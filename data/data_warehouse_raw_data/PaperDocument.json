[{"_id": "67847ebb342d0974a0ada2cf", "content": "## LLM.int8() : 8-bit Matrix Multiplication\n\n## for Transformers at Scale\n\nTim Dettmers \u03bb \u2217\n\nMike Lewis \u2020\n\nYounes Belkada \u00a7\u2213\n\nLuke Zettlemoyer \u2020 \u03bb\n\nUniversity of Washington Facebook AI Research \u2020 Hugging Face \u00a7 ENS Paris-Saclay \u2213\n\n\u03bb\n\n## Abstract\n\nLarge language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8() . We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open source our software.\n\n## 1 Introduction\n\nLarge pretrained language models are widely adopted in NLP (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020; Zhang et al., 2022) but require significant memory for inference. For large transformer language models at and beyond 6.7B parameters, the feed-forward and attention projection layers and their matrix multiplication operations are responsible for 95% 2 of consumed parameters and 65-85% of all computation (Ilharco et al., 2020). One way to reduce the size of the parameters is to quantize them to less bits and use low-bit-precision matrix multiplication. With this goal in mind, 8-bit quantization methods for transformers have been developed (Chen et al., 2020; Lin et al., 2020; Zafrir et al., 2019; Shen et al., 2020). While these methods reduce memory use, they degrade performance, usually require tuning quantization further after training, and have only been studied for models with less than 350M parameters. Degradation-free quantization up to 350M parameters is poorly understood, and multi-billion parameter quantization remains an open challenge.\n\nIn this paper, we present the first multi-billion-scale Int8 quantization procedure for transformers that does not incur any performance degradation. Our procedure makes it possible to load a 175B parameter transformer with 16 or 32-bit weights, convert the feed-forward and attention projection layers to 8-bit, and use the resulting model immediately for inference without any performance degradation. We achieve this result by solving two key challenges: the need for higher quantization precision at scales beyond 1B parameters and the need to explicitly represent the sparse but systematic large magnitude outlier features that ruin quantization precision once they emerge in all transformer layers starting at scales of 6.7B parameters. This loss of precision is reflected in C4 evaluation perplexity (Section 3) as well as zeroshot accuracy as soon as these outlier features emerge, as shown in Figure 1.\n\nWe show that with the first part of our method, vector-wise quantization, it is possible to retain performance at scales up to 2.7B parameters. For vector-wise quantization, matrix multiplication can be seen as a sequence of independent inner products of row and column vectors. As such, we can use a separate quantization normalization constant for each inner product to improve quantization precision. We can recover the output of the matrix multiplication by denormalizing by the outer product of column and row normalization constants before we perform the next operation.\n\nTo scale beyond 6.7B parameters without performance degradation, it is critical to understand the emergence of extreme outliers in the feature dimensions of the hidden states during inference. To this end, we provide a new descriptive analysis which shows that large features with magnitudes up to 20x larger than in other dimensions first appear in about 25% of all transformer layers and then gradually spread to other layers as we scale transformers to 6B parameters. At around 6.7B parameters, a phase shift occurs, and all transformer layers and 75% of all sequence dimensions are affected by extreme\n\nFigure 1: OPT model mean zeroshot accuracy for WinoGrande, HellaSwag, PIQA, and LAMBADA datasets. Shown is the 16-bit baseline, the most precise previous 8-bit quantization method as a baseline, and our new 8-bit quantization method, LLM.int8(). We can see once systematic outliers occur at a scale of 6.7B parameters, regular quantization methods fail, while LLM.int8() maintains 16-bit accuracy.\n\n<!-- image -->\n\nmagnitude features. These outliers are highly systematic: at the 6.7B scale, 150,000 outliers occur per sequence, but they are concentrated in only 6 feature dimensions across the entire transformer. Setting these outlier feature dimensions to zero decreases top-1 attention softmax probability mass by more than 20% and degrades validation perplexity by 600-1000% despite them only making up about 0.1% of all input features. In contrast, removing the same amount of random features decreases the probability by a maximum of 0.3% and degrades perplexity by about 0.1%.\n\nTo support effective quantization with such extreme outliers, we develop mixed-precision decomposition, the second part of our method. We perform 16-bit matrix multiplication for the outlier feature dimensions and 8-bit matrix multiplication for the other 99.9% of the dimensions. We name the combination of vector-wise quantization and mixed precision decomposition, LLM.int8() . We show that by using LLM.int8(), we can perform inference in LLMs with up to 175B parameters without any performance degradation. Our method not only provides new insights into the effects of these outliers on model performance but also makes it possible for the first time to use very large models, for example, OPT-175B/BLOOM, on a single server with consumer GPUs. While our work focuses on making large language models accessible without degradation, we also show in Appendix D that we maintain end-to-end inference runtime performance for large models, such as BLOOM-176B and provide modest matrix multiplication speedups for GPT-3 models of size 6.7B parameters or larger. We open-source our software 3 and release a Hugging Face Transformers (Wolf et al., 2019) integration making our method available to all hosted Hugging Face Models that have linear layers.\n\nFigure 2: Schematic of LLM.int8(). Given 16-bit floating-point inputs X f 16 and weights W f 16 , the features and weights are decomposed into sub-matrices of large magnitude features and other values. The outlier feature matrices are multiplied in 16-bit. All other values are multiplied in 8-bit. We perform 8-bit vector-wise multiplication by scaling by row and column-wise absolute maximum of C x and C w and then quantizing the outputs to Int8. The Int32 matrix multiplication outputs Out i 32 are dequantization by the outer product of the normalization constants C x \u2297 C w . Finally, both outlier and regular outputs are accumulated in 16-bit floating point outputs.\n\n<!-- image -->\n\n## 2 Background\n\nIn this work, push quantization techniques to their breaking point by scaling transformer models. We are interested in two questions: at which scale and why do quantization techniques fail and how does this related to quantization precision? To answer these questions we study high-precision asymmetric quantization (zeropoint quantization) and symmetric quantization (absolute maximum quantization). While zeropoint quantization offers high precision by using the full bit-range of the datatype, it is rarely used due to practical constraints. Absolute maximum quantization is the most commonly used technique.\n\n## 2.1 8-bit Data Types and Quantization\n\nAbsmax quantization scales inputs into the 8-bit range [ -127 , 127] by multiplying with s x f 16 which is 127 divided by the absolute maximum of the entire tensor. This is equivalent to dividing by the infinity norm and multiplying by 127. As such, for an FP16 input matrix X f 16 \u2208 R s \u00d7 h Int8 absmax quantization is given by:\n\nX i 8 = \u230a 127 \u00b7 X f 16 max ij ( | X f 16 ij | ) \u2309 = \u230a 127 \u2016 X f 16 \u2016 \u221e X f 16 \u2309 = \u230a s x f 16 X f 16 \u2309 ,\n\nwhere glyph[floorleft]glyph[ceilingright] indicates rounding to the nearest integer.\n\nZeropoint quantization shifts the input distribution into the full range [ -127 , 127] by scaling with the normalized dynamic range nd x and then shifting by the zeropoint zp x . With this affine transformation, any input tensors will use all bits of the data type, thus reducing the quantization error for asymmetric distributions . For example, for ReLU outputs, in absmax quantization all values in [ -127 , 0) go unused, whereas in zeropoint quantization the full [ -127 , 127] range is used. Zeropoint quantization is given by the following equations:\n\nnd x f 16 = 2 \u00b7 127 max ij ( X ij f 16 ) -min ij ( X ij f 16 ) (1)\n\nzp x i 16 = \u230a X f 16 \u00b7 min ij ( X ij f 16 ) \u2309 (2)\n\nX i 8 = \u230a nd x f 16 X f 16 \u2309 (3)\n\nTo use zeropoint quantization in an operation we feed both the tensor X i 8 and the zeropoint zp x i 16 into a special instruction 4 which adds zp x i 16 to each element of X i 8 before performing a 16-bit integer operation. For example, to multiply two zeropoint quantized numbers A i 8 and B i 8 along with their zeropoints zp a i 16 and zp b i 16 we calculate:\n\nC i 32 = multiply i 16 ( A zp a i 16 , B zp b i 16 ) = ( A i 8 + zp a i 16 )( B i 8 + zp b i 16 ) (4)\n\nwhere unrolling is required if the instruction multiply i 16 is not available such as on GPUs or TPUs:\n\nC i 32 = A i 8 B i 8 + A i 8 zp b i 16 + B i 8 zp a i 16 + zp a i 16 zp b i 16 , (5)\n\nwhere A i 8 B i 8 is computed with Int8 precision while the rest is computed in Int16/32 precision. As such, zeropoint quantization can be slow if the multiply i 16 instruction is not available. In both cases, the outputs are accumulated as a 32-bit integer C i 32 . To dequantize C i 32 , we divide by the scaling constants nd a f 16 and nd b f 16 .\n\nInt8 Matrix Multiplication with 16-bit Float Inputs and Outputs. Given hidden states X f 16 \u2208 R s \u00d7 h and weights W f 16 \u2208 R h \u00d7 o with sequence dimension s , feature dimension h , and output dimension o we perform 8-bit matrix multiplication with 16-bit inputs and outputs as follows:\n\nX f 16 W f 16 = C f 16 \u2248 1 c x f 16 c w f 16 C i 32 = S f 16 \u00b7 C i 32 \u2248 S f 16 \u00b7 A i 8 B i 8 = S f 16 \u00b7 Q ( A f 16 ) Q ( B f 16 ) , (6)\n\nWhere Q ( \u00b7 ) is either absmax or zeropoint quantization and c x f 16 and c w f 16 are the respective tensorwise scaling constants s x and s w for absmax or nd x and nd w for zeropoint quantization.\n\n## 3 Int8 Matrix Multiplication at Scale\n\nThe main challenge with quantization methods that use a single scaling constant per tensor is that a single outlier can reduce the quantization precision of all other values. As such, it is desirable to have multiple scaling constants per tensor, such as block-wise constants (Dettmers et al., 2022), so that the effect of that outliers is confined to each block. We improve upon one of the most common ways of blocking quantization, row-wise quantization (Khudia et al., 2021), by using vector-wise quantization, as described in more detail below.\n\nTo handle the large magnitude outlier features that occur in all transformer layers beyond the 6.7B scale, vector-wise quantization is no longer sufficient. For this purpose, we develop mixedprecision decomposition, where the small number of large magnitude feature dimensions ( \u2248 0.1%) are represented in 16-bit precision while the other 99.9% of values are multiplied in 8-bit. Since most entries are still represented in low-precision, we retain about 50% memory reduction compared to 16-bit. For example, for BLOOM-176B, we reduce the memory footprint of the model by 1.96x.\n\nVector-wise quantization and mixed-precision decomposition are shown in Figure 2. The LLM.int8() method is the combination of absmax vector-wise quantization and mixed precision decomposition.\n\n## 3.1 Vector-wise Quantization\n\nOne way to increase the number of scaling constants for matrix multiplication is to view matrix multiplication as a sequence of independent inner products. Given the hidden states X f 16 \u2208 R b \u00d7 h and weight matrix W f 16 \u2208 R h \u00d7 o , we can assign a different scaling constant c x f 16 to each row of X f 16 and c w to each column of W f 16 . To dequantize, we denormalize each inner product result by 1 / ( c x f 16 c w f 16 ) . For the whole matrix multiplication this is equivalent to denormalization by the outer product c x f 16 \u2297 c w f 16 , where c x \u2208 R s and c w \u2208 R o . As such the full equation for matrix multiplication with row and column constants is given by:\n\nC f 16 \u2248 1 c x f 16 \u2297 c w f 16 C i 32 = S \u00b7 C i 32 = S \u00b7 A i 8 B i 8 = S \u00b7 Q ( A f 16 ) Q ( B f 16 ) , (7)\n\nwhich we term vector-wise quantization for matrix multiplication.\n\n## 3.2 The Core of LLM.int8(): Mixed-precision Decomposition\n\nIn our analysis, we demonstrate that a significant problem for billion-scale 8-bit transformers is that they have large magnitude features ( columns ), which are important for transformer performance and require high precision quantization. However, vector-wise quantization, our best quantization technique, quantizes each row for the hidden state, which is ineffective for outlier features. Luckily, we see that these outlier features are both incredibly sparse and systematic in practice, making up only about 0.1% of all feature dimensions, thus allowing us to develop a new decomposition technique that focuses on high precision multiplication for these particular dimensions.\n\nWe find that given input matrix X f 16 \u2208 R s \u00d7 h , these outliers occur systematically for almost all sequence dimensions s but are limited to specific feature/hidden dimensions h . As such, we propose mixed-precision decomposition for matrix multiplication where we separate outlier feature dimensions into the set O = { i | i \u2208 Z , 0 \u2264 i \u2264 h } , which contains all dimensions of h which have at least one outlier with a magnitude larger than the threshold \u03b1 . In our work, we find that \u03b1 = 6 . 0 is sufficient to reduce transformer performance degradation close to zero. Using Einstein notation where all indices are superscripts, given the weight matrix W f 16 \u2208 R h \u00d7 o , mixed-precision decomposition for matrix multiplication is defined as follows:\n\nC f 16 \u2248 \u2211 h \u2208 O X h f 16 W h f 16 + S f 16 \u00b7 \u2211 h glyph[negationslash]\u2208 O X h i 8 W h i 8 (8)\n\nwhere S f 16 is the denormalization term for the Int8 inputs and weight matrices X i 8 and W i 8 .\n\nThis separation into 8-bit and 16-bit allows for high-precision multiplication of outliers while using memory-efficient matrix multiplication with 8-bit weights of more than 99.9% of values. Since the number of outlier feature dimensions is not larger than 7 ( | O | \u2264 7 ) for transformers up to 13B parameters, this decomposition operation only consumes about 0.1% additional memory.\n\n## 3.3 Experimental Setup\n\nWe measure the robustness of quantization methods as we scale the size of several publicly available pretrained language models up to 175B parameters. The key question is not how well a quantization method performs for a particular model but the trend of how such a method performs as we scale.\n\nWe use two setups for our experiments. One is based on language modeling perplexity, which we find to be a highly robust measure that is very sensitive to quantization degradation. We use this setup to compare different quantization baselines. Additionally, we evaluate zeroshot accuracy degradation on OPT models for a range of different end tasks, where we compare our methods with a 16-bit baseline.\n\nFor the language modeling setup, we use dense autoregressive transformers pretrained in fairseq (Ott et al., 2019) ranging between 125M and 13B parameters. These transformers have been pretrained on Books (Zhu et al., 2015), English Wikipedia, CC-News (Nagel, 2016), OpenWebText (Gokaslan and Cohen, 2019), CC-Stories (Trinh and Le, 2018), and English CC100 (Wenzek et al., 2020). For more information on how these pretrained models are trained, see Artetxe et al. (2021).\n\nTo evaluate the language modeling degradation after Int8 quantization, we evaluate the perplexity of the 8-bit transformer on validation data of the C4 corpus (Raffel et al., 2019) which is a subset of the Common Crawl corpus. 5 We use NVIDIA A40 GPUs for this evaluation.\n\nTo measure degradation in zeroshot performance, we use OPT models (Zhang et al., 2022), and we evaluate these models on the EleutherAI language model evaluation harness (Gao et al., 2021).\n\n## 3.4 Main Results\n\nThe main language modeling perplexity results on the 125M to 13B Int8 models evaluated on the C4 corpus can be seen in Table 1. We see that absmax, row-wise, and zeropoint quantization fail as we scale, where models after 2.7B parameters perform worse than smaller models. Zeropoint quantization fails instead beyond 6.7B parameters. Our method, LLM.int8(), is the only method that preserves perplexity. As such, LLM.int8() is the only method with a favorable scaling trend.\n\nTable 1: C4 validation perplexities of quantization methods for different transformer sizes ranging from 125M to 13B parameters. We see that absmax, row-wise, zeropoint, and vector-wise quantization leads to significant performance degradation as we scale, particularly at the 13B mark where 8-bit 13B perplexity is worse than 8-bit 6.7B perplexity. If we use LLM.int8(), we recover full perplexity as we scale. Zeropoint quantization shows an advantage due to asymmetric quantization but is no longer advantageous when used with mixed-precision decomposition.\n\n| Parameters                                  |   125M |   1.3B |   2.7B |   6.7B |   13B |\n|---------------------------------------------|--------|--------|--------|--------|-------|\n| 32-bit Float                                |  25.65 |  15.91 |  14.43 |  13.3  | 12.45 |\n| Int8 absmax                                 |  87.76 |  16.55 |  15.11 |  14.59 | 19.08 |\n| Int8 zeropoint                              |  56.66 |  16.24 |  14.76 |  13.49 | 13.94 |\n| Int8 absmax row-wise                        |  30.93 |  17.08 |  15.24 |  14.13 | 16.49 |\n| Int8 absmax vector-wise                     |  35.84 |  16.82 |  14.98 |  14.13 | 16.48 |\n| Int8 zeropoint vector-wise                  |  25.72 |  15.94 |  14.36 |  13.38 | 13.47 |\n| Int8 absmax row-wise + decomposition        |  30.76 |  16.19 |  14.65 |  13.25 | 12.46 |\n| Absmax LLM.int8() (vector-wise + decomp)    |  25.83 |  15.93 |  14.44 |  13.24 | 12.45 |\n| Zeropoint LLM.int8() (vector-wise + decomp) |  25.69 |  15.92 |  14.43 |  13.24 | 12.45 |\n\nWhen we look at the scaling trends of zeroshot performance of OPT models on the EleutherAI language model evaluation harness in Figure 1, we see that LLM.int8() maintains full 16-bit performance as we scale from 125M to 175B parameters. On the other hand, the baseline, 8-bit absmax vector-wise quantization, scales poorly and degenerates into random performance.\n\nAlthough our primary focus is on saving memory, we also measured the run time of LLM.int8(). The quantization overhead can slow inference for models with less than 6.7B parameters, as compared to a FP16 baseline. However, models of 6.7B parameters or less fit on most GPUs and quantization is less needed in practice. LLM.int8() run times is about two times faster for large matrix multiplications equivalent to those in 175B models. Appendix D provides more details on these experiments.\n\n## 4 Emergent Large Magnitude Features in Transformers at Scale\n\nAs we scale transformers, outlier features with large magnitudes emerge and strongly affect all layers and their quantization. Given a hidden state X \u2208 R s \u00d7 h where s is the sequence/token dimension and h the hidden/feature dimension, we define a feature to be a particular dimension h i . Our analysis looks at a particular feature dimension h i across all layers of a given transformer.\n\nWe find that outlier features strongly affect attention and the overall predictive performance of transformers. While up to 150k outliers exist per 2048 token sequence for a 13B model, these outlier features are highly systematic and only representing at most 7 unique feature dimensions h i . Insights from this analysis were critical to developing mixed-precision decomposition. Our analysis explains the advantages of zeropoint quantization and why they disappear with the use of mixed-precision decomposition and the quantization performance of small vs. large models.\n\n## 4.1 Finding Outlier Features\n\nThe difficulty with the quantitative analysis of emergent phenomena is two-fold. We aim to select a small subset of features for analysis such that the results are intelligible and not to complex while also capturing important probabilistic and structured patterns. We use an empirical approach to find these constraints. We define outliers according to the following criteria: the magnitude of the feature is at least 6.0, affects at least 25% of layers, and affects at least 6% of the sequence dimensions.\n\nMore formally, given a transformer with L layers and hidden state X l \u2208 R s \u00d7 h , l = 0 ...L where s is the sequence dimension and h the feature dimension, we define a feature to be a particular dimension h i in any of the hidden states X l i . We track dimensions h i , 0 \u2264 i \u2264 h , which have at least one value with a magnitude of \u03b1 \u2265 6 and we only collect statistics if these outliers occur in the same feature dimension h i in at least 25% of transformer layers 0 ...L and appear in at least 6% of all sequence dimensions s across all hidden states X l . Since feature outliers only occur in attention projection\n\n<!-- image -->\n\nFigure 3: Percentage of layers and all sequence dimensions affected by large magnitude outlier features across the transformer by (a) model size or (b) C4 perplexity. Lines are B-spline interpolations of 4 and 9 linear segments for (a) and (b). Once the phase shift occurs, outliers are present in all layers and in about 75% of all sequence dimensions. While (a) suggest a sudden phase shift in parameter size, (b) suggests a gradual exponential phase shift as perplexity decreases. The stark shift in (a) co-occurs with the sudden degradation of performance in quantization methods.\n\n<!-- image -->\n\n(key/query/value/output) and the feedforward network expansion layer (first sub-layer), we ignore the attention function and the FFN contraction layer (second sub-layer) for this analysis.\n\nOur reasoning for these thresholds is as follows. We find that using mixed-precision decomposition, perplexity degradation stops if we treat any feature with a magnitude 6 or larger as an outlier feature. For the number of layers affected by outliers, we find that outlier features are systematic in large models: they either occur in most layers or not at all. On the other hand, they are probabilistic in small models: they occur sometimes in some layers for each sequence. As such, we set our threshold for how many layers need to be affected to detect an outlier feature in such a way as to limit detection to a single outlier in our smallest model with 125M parameters. This threshold corresponds to that at least 25% of transformer layers are affected by an outlier in the same feature dimension. The second most common outlier occurs in only a single layer ( 2% of layers), indicating that this is a reasonable threshold. We use the same procedure to find the threshold for how many sequence dimensions are affected by outlier features in our 125M model: outliers occur in at least 6% of sequence dimensions.\n\nWe test models up to a scale of 13B parameters. To make sure that the observed phenomena are not due to bugs in software, we evaluate transformers that were trained in three different software frameworks. We evaluate four GPT-2 models which use OpenAI software, five Meta AI models that use Fairseq (Ott et al., 2019), and one EleutherAI model GPT-J that uses Tensorflow-Mesh (Shazeer et al., 2018). More details can be found in Appendix C. We also perform our analysis in two different inference software frameworks: Fairseq and Hugging Face Transformers (Wolf et al., 2019).\n\n## 4.2 Measuring the Effect of Outlier Features\n\nTo demonstrate that the outlier features are essential for attention and predictive performance, we set the outlier features to zero before feeding the hidden states X l into the attention projection layers and then compare the top-1 softmax probability with the regular softmax probability with outliers. We do this for all layers independently, meaning we forward the regular softmax probabilities values to avoid cascading errors and isolate the effects due to the outlier features. We also report the perplexity degradation if we remove the outlier feature dimension (setting them to zero) and propagate these altered, hidden states through the transformer. As a control, we apply the same procedure for random non-outlier feature dimensions and note attention and perplexity degradation.\n\nOur main quantitative results can be summarized as four main points.\n\n<!-- image -->\n\nFigure 4: The median magnitude of the largest outlier feature in (a) indicates a sudden shift in outlier size. This appears to be the prime reason why quantization methods fail after emergence. While the number of outlier feature dimensions is only roughly proportional to model size, (b) shows that the number of outliers is strictly monotonic with respect to perplexity across all models analyzed. Lines are B-spline interpolations of 9 linear segments.\n\n<!-- image -->\n\n- (1) When measured by the number of parameters, the emergence of large magnitude features across all layers of a transformer occurs suddenly between 6B and 6.7B parameters as shown in Figure 3a as the percentage of layers affected increases from 65% to 100%. The number of sequence dimensions affected increases rapidly from 35% to 75%. This sudden shift co-occurs with the point where quantization begins to fail.\n- (2) Alternatively, when measured by perplexity, the emergence of large magnitude features across all layers of the transformer can be seen as emerging smoothly according to an exponential function of decreasing perplexity, as seen in Figure 3b. This indicates that there is nothing sudden about emergence and that we might be able to detect emergent features before a phase shift occurs by studying exponential trends in smaller models. This also suggests that emergence is not only about model size but about perplexity, which is related to multiple additional factors such as the amount of training data used, and data quality (Hoffmann et al., 2022; Henighan et al., 2020).\n- (3) Median outlier feature magnitude rapidly increases once outlier features occur in all layers of the transformer, as shown in Figure 4a. The large magnitude of outliers features and their asymmetric distribution disrupts Int8 quantization precision. This is the core reason why quantization methods fail starting at the 6.7B scale - the range of the quantization distribution is too large so that most quantization bins are empty and small quantization values are quantized to zero, essentially extinguishing information. We hypothesize that besides Int8 inference, regular 16-bit floating point training becomes unstable due to outliers beyond the 6.7B scale - it is easy to exceed the maximum 16-bit value 65535 by chance if you multiply by vectors filled with values of magnitude 60.\n- (4) The number of outliers features increases strictly monotonically with respect to decreasing C4 perplexity as shown in Figure 4b, while a relationship with model size is non-monotonic. This indicates that model perplexity rather than mere model size determines the phase shift. We hypothesize that model size is only one important covariate among many that are required to reach emergence.\n\nThese outliers features are highly systematic after the phase shift occurred. For example, for a 6.7B transformer with a sequence length of 2048, we find about 150k outlier features per sequence for the entire transformer, but these features are concentrated in only 6 different hidden dimensions.\n\nThese outliers are critical for transformer performance. If the outliers are removed, the mean top-1 softmax probability is reduced from about 40% to about 20%, and validation perplexity increases by 600-1000% even though there are at most 7 outlier feature dimensions. When we remove 7 random feature dimensions instead, the top-1 probability decreases only between 0.02-0.3%, and perplexity increases by 0.1%. This highlights the critical nature of these feature dimensions. Quantization precision for these outlier features is paramount as even tiny errors greatly impact model performance.\n\n## 4.3 Interpretation of Quantization Performance\n\nOur analysis shows that outliers in particular feature dimensions are ubiquitous in large transformers, and these feature dimensions are critical for transformer performance. Since row-wise and vectorwise quantization scale each hidden state sequence dimension s (rows) and because outliers occur in the feature dimension h (columns), both methods cannot deal with these outliers effectively. This is why absmax quantization methods fail quickly after emergence.\n\nHowever, almost all outliers have a strict asymmetric distribution: they are either solely positive or negative (see Appendix C). This makes zeropoint quantization particularly effective for these outliers, as zeropoint quantization is an asymmetric quantization method that scales these outliers into the full [ -127 , 127] range. This explains the strong performance in our quantization scaling benchmark in Table 1. However, at the 13B scale, even zeropoint quantization fails due to accumulated quantization errors and the quick growth of outlier magnitudes, as seen in Figure 4a.\n\nIf we use our full LLM.int8() method with mixed-precision decomposition, the advantage of zeropoint quantization disappears indicating that the remaining decomposed features are symmetric. However, vector-wise still has an advantage over row-wise quantization, indicating that the enhanced quantization precision of the model weights is needed to retain full precision predictive performance.\n\n## 5 Related work\n\nThere is closely related work on quantization data types and quantization of transformers, as described below. Appendix B provides further related work on quantization of convolutional networks.\n\n8-bit Data Types. Our work studies quantization techniques surrounding the Int8 data type, since it is currently the only 8-bit data type supported by GPUs. Other common data types are fixed point or floating point 8-bit data types (FP8). These data types usually have a sign bit and different exponent and fraction bit combinations. For example, a common variant of this data type has 5 bits for the exponent and 2 bits for the fraction (Wang et al., 2018; Sun et al., 2019; Cambier et al., 2020; Mellempudi et al., 2019) and uses either no scaling constants or zeropoint scaling. These data types have large errors for large magnitude values since they have only 2 bits for the fraction but provide high accuracy for small magnitude values. Jin et al. (2022) provide an excellent analysis of when certain fixed point exponent/fraction bit widths are optimal for inputs with a particular standard deviation. We believe FP8 data types offer superior performance compared to the Int8 data type, but currently, neither GPUs nor TPUs support this data type.\n\nOutlier Features in Language Models. Large magnitude outlier features in language models have been studied before (Timkey and van Schijndel, 2021; Bondarenko et al., 2021; Wei et al., 2022; Luo et al., 2021). Previous work proved the theoretical relationship between outlier appearance in transformers and how it relates to layer normalization and the token frequency distribution (Gao et al., 2019). Similarly, Kovaleva et al. (2021) attribute the appearance of outliers in BERT model family to LayerNorm, and Puccetti et al. (2022) show empirically that outlier emergence is related to the frequency of tokens in the training distribution. We extend this work further by showing how the scale of autoregressive models relates to the emergent properties of these outlier features, and showing how appropriately modeling outliers is critical to effective quantization.\n\nMulti-billion Scale Transformer Quantization. There are two methods that were developed in parallel to ours: nuQmm (Park et al., 2022) and ZeroQuant (Yao et al., 2022). Both use the same quantization scheme: group-w2ise quantization, which has even finer quantization normalization constant granularity than vector-wise quantization. This scheme offers higher quantization precision but also requires custom CUDA kernels. Both nuQmm and ZeroQuant aim to accelerate inference and reduce the memory footprint while we focus on preserving predictive performance under an 8-bit memory footprint. The largest models that nuQmm and ZeroQuant evaluate are 2.7B and 20B parameter transformers, respectively. ZeroQuant achieves zero-degradation performance for 8-bit quantization of a 20B model. We show that our method allows for zero-degradation quantization of models up to 176B parameters. Both nuQmm and ZeroQuant suggest that finer quantization granularity can be an effective means to quantize large models. These methods are complementary with LLM.int8(). Another parallel work is GLM-130B which uses insights from our work to achieve zero-degradation 8-bit quantization (Zeng et al., 2022). GLM-130B performs full 16-bit precision matrix multiplication with 8-bit weight storage.\n\n## 6 Discussion and Limitations\n\nWe have demonstrated for the first time that multi-billion parameter transformers can be quantized to Int8 and used immediately for inference without performance degradation. We achieve this by using our insights from analyzing emergent large magnitude features at scale to develop mixed-precision decomposition to isolate outlier features in a separate 16-bit matrix multiplication. In conjunction with vector-wise quantization that yields our method, LLM.int8(), which we show empirically can recover the full inference performance of models with up to 175B parameters.\n\nThe main limitation of our work is that our analysis is solely on the Int8 data type, and we do not study 8-bit floating-point (FP8) data types. Since current GPUs and TPUs do not support this data type, we believe this is best left for future work. However, we also believe many insights from Int8 data types will directly translate to FP8 data types. Another limitation is that we only study models with up to 175B parameters. While we quantize a 175B model to Int8 without performance degradation, additional emergent properties might disrupt our quantization methods at larger scales.\n\nA third limitation is that we do not use Int8 multiplication for the attention function. Since our focus is on reducing the memory footprint and the attention function does not use any parameters, it was not strictly needed. However, an initial exploration of this problem indicated that a solution required additional quantization methods beyond those we developed here, and we leave this for future work.\n\nA final limitation is that we focus on inference but do not study training or finetuning. We provide an initial analysis of Int8 finetuning and training at scale in Appendix E. Int8 training at scale requires complex trade-offs between quantization precision, training speed, and engineering complexity and represents a very difficult problem. We again leave this to future work.\n\nTable 2: Different hardware setups and which methods can be run in 16-bit vs. 8-bit precision. We can see that our 8-bit method makes many models accessible that were not accessible before, in particular, OPT-175B/BLOOM.\n\n|                  |             |            | Largest Model that can be run   | Largest Model that can be run   |\n|------------------|-------------|------------|---------------------------------|---------------------------------|\n| Class            | Hardware    | GPU Memory | 8-bit                           | 16-bit                          |\n| Enterprise       | 8x A100     | 80 GB      | OPT-175B / BLOOM                | OPT-175B / BLOOM                |\n| Enterprise       | 8x A100     | 40 GB      | OPT-175B / BLOOM                | OPT-66B                         |\n| Academic server  | 8x RTX 3090 | 24 GB      | OPT-175B / BLOOM                | OPT-66B                         |\n| Academic desktop | 4x RTX 3090 | 24 GB      | OPT-66B                         | OPT-30B                         |\n| Paid Cloud       | Colab Pro   | 15 GB      | OPT-13B                         | GPT-J-6B                        |\n| Free Cloud       | Colab       | 12 GB      | T0/T5-11B                       | GPT-2 1.3B                      |\n\n## 7 Broader Impacts\n\nThe main impact of our work is enabling access to large models that previously could not fit into GPU memory. This enables research and applications which were not possible before due to limited GPU memory, in particular for researchers with the least resources. See Table 3 for model/GPU combinations which are now accessible without performance degradation. However, our work also enables resource-rich organizations with many GPUs to serve more models on the same number of GPUs, which might increase the disparities between resource-rich and poor organizations.\n\nIn particular, we believe that the public release of large pretrained models, for example, the recent Open Pretrained Transformers (OPT) (Zhang et al., 2022), along with our new Int8 inference for zeroand few-shot prompting, will enable new research for academic institutions that was not possible before due to resource constraints. The widespread accessibility of such large-scale models will likely have both beneficial and detrimental effects on society that are difficult to predict.\n\nAcknowledgments We thank Ofir Press, Gabriel Ilharco, Daniel Jiang, Mitchell Wortsman, Ari Holtzman, Mitchell Gordon for their feedback on drafts of this work. We thank JustHeuristic (Yozh) and Titus von K\u00f6ller for help with Hugging Face Transformers integration.\n\n## References\n\n| Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V., Du, J., Iyer, S., Pasunuru, R., et al. (2021). Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684 .                                                                                                                                                                             |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Bai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu, Q., Lyu, M. R., and King, I. (2021). Binarybert: Pushing the limit of bert quantization. ArXiv , abs/2012.15701.                                                                                                                                                                                                                                    |\n| Bondarenko, Y., Nagel, M., and Blankevoort, T. (2021). Understanding and overcoming the challenges of efficient transformer quantization. arXiv preprint arXiv:2109.12948 .                                                                                                                                                                                                                                          |\n| Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165 .                                                                                                                                                                                                |\n| Cambier, L., Bhiwandiwalla, A., Gong, T., Elibol, O. H., Nekuii, M., and Tang, H. (2020). Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.                                                                                    |\n| Chen, J., Gai, Y., Yao, Z., Mahoney, M. W., and Gonzalez, J. E. (2020). A statistical framework for low-bitwidth training of deep neural networks. Advances in Neural Information Processing Systems , 33:883-894.                                                                                                                                                                                                   |\n| Choi, J., Venkataramani, S., Srinivasan, V., Gopalakrishnan, K., Wang, Z., and Chuang, P. (2019). Accurate and efficient 2-bit quantized neural networks. In Talwalkar, A., Smith, V., and Zaharia, M., editors, Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA, USA, March 31 - April 2, 2019 . mlsys.org.                                                                              |\n| Courbariaux, M. and Bengio, Y. (2016). Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1. CoRR , abs/1602.02830.                                                                                                                                                                                                                                                         |\n| Courbariaux, M., Bengio, Y., and David, J. (2015). Binaryconnect: Training deep neural networks with binary weights during propagations. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pages 3123-3131. |\n| Courbariaux, M., Bengio, Y., and David, J.-P. (2014). Training deep neural networks with low precision multiplications. arXiv preprint arXiv:1412.7024 .                                                                                                                                                                                                                                                             |\n| Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. (2022). 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR .                                                                                                                                                                                                                                    |\n| Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .                                                                                                                                                                                                                                     |\n| Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. (2019). Hawq: Hessian aware quan- tization of neural networks with mixed-precision. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 293-302.                                                                                                                                                                      |\n| Esser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. (2019). Learned step size quantization. arXiv preprint arXiv:1902.08153 .                                                                                                                                                                                                                                                               |\n| Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R., Jegou, H., and Joulin, A. (2020). Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320 .                                                                                                                                                                                                                        |\n| Gao, J., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y. (2019). Representation degeneration problem in training natural language generation models. arXiv preprint arXiv:1907.12009 .                                                                                                                                                                                                                            |\n| Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2021). A framework for few-shot language model evaluation.                                                                                                                                                    |\n\n| Gokaslan, A. and Cohen, V. (2019). Openwebtext corpus. urlhttp://Skylion007. github. io/OpenWebTextCorpus .                                                                                                                                                                                                                                   |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                                                                                                                                                                                                                                               |\n| Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal, P., Gray, S., et al. (2020). Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701 .                                                                                                                    |\n| Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language arXiv preprint arXiv:2203.15556 . Ilharco, G., Ilharco, C., Turc, I., Dettmers, T., Ferreira, F., and Lee, K. (2020). High performance nat- |\n| ural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts , pages 24-27, Online. Association for Computational Jin, Q., Ren, J., Zhuang, R., Hanumante, S., Li, Z., Chen, Z., Wang, Y., Yang, K., and Tulyakov,                                                 |\n| Khudia, D., Huang, J., Basu, P., Deng, S., Liu, H., Park, J., and Smelyanskiy, M. (2021). Fbgemm: En- abling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615 .                                                                                                                                        |\n| Kovaleva, O., Kulshreshtha, S., Rogers, A., and Rumshisky, A. (2021). Bert busters: Outlier dimensions that disrupt transformers. arXiv preprint arXiv:2105.06990 .                                                                                                                                                                           |\n| Li, R., Wang, Y., Liang, F., Qin, H., Yan, J., and Fan, R. (2019). Fully quantized network for object IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long                                                                                                                                                             |\n| detection. In Beach, CA, USA, June 16-20, 2019 , pages 2810-2819. Computer Vision Foundation / IEEE. Lin, Y., Li, Y., Liu, T., Xiao, T., Liu, T., and Zhu, J. (2020). Towards fully 8-bit integer inference for the transformer model. arXiv preprint arXiv:2009.08034 .                                                                      |\n| Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint                                                                                                                                              |\n| Luo, Z., Kulmizev, A., and Mao, X. (2021). Positional artefacts propagate through masked language Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Conference on Natural Language Processing , pages 5312-5327, Online. Association for Computational Linguistics.  |\n| Mach\u00e1\u02c7cek, M. and Bojar, O. (2014). Results of the wmt14 metrics shared task. In Proceedings of the Ninth Workshop on Statistical Machine Translation , pages 293-301.                                                                                                                                                                        |\n| Mellempudi, N., Srinivasan, S., Das, D., and Kaul, B. (2019). Mixed precision training with 8-bit floating point. CoRR , abs/1905.12334.                                                                                                                                                                                                      |\n| Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. (2019). arXiv preprint arXiv:1904.01038 .                                                                                                                                                                                                            |\n| Ott, M., Edunov, S., Grangier, D., and Auli, M. (2018). Scaling neural machine translation. arXiv                                                                                                                                                                                                                                             |\n\n| Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. (2022). nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 .                                                                                                                                                                                                                                           |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Puccetti, G., Rogers, A., Drozd, A., and Dell'Orletta, F. (2022). Outliers dimensions that disrupt transformers are driven by frequency. arXiv preprint arXiv:2205.11380 .                                                                                                                                                                                                                                                                    |\n| Qin, H., Gong, R., Liu, X., Bai, X., Song, J., and Sebe, N. (2020). Binary neural networks: A survey.                                                                                                                                                                                                                                                                                                                                         |\n| Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are                                                                                                                                                                                                                                                                                                                                           |\n| Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 . Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. (2016). Xnor-net: Imagenet classification                                                                                                        |\n| using binary convolutional neural networks. In Leibe, B., Matas, J., Sebe, N., and Welling, M., editors, Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV , volume 9908 of Lecture Notes in Computer Science , pages 525-542. Springer. Sennrich, R., Haddow, B., and Birch, A. (2016). Edinburgh neural machine translation systems for                         |\n| wmt 16. arXiv preprint arXiv:1606.02891 . Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., et al. (2018). Mesh-tensorflow: Deep learning for supercomputers. Advances in neural information processing systems , 31.                                                                                                                                                   |\n| Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. (2020).                                                                                                                                                                                                                                                                                                                                             |\n| Sun, X., Choi, J., Chen, C., Wang, N., Venkataramani, S., Srinivasan, V., Cui, X., Zhang, W., and                                                                                                                                                                                                                                                                                                                                             |\n| Gopalakrishnan, K. (2019). Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,                                                           |\n| Timkey, W. and van Schijndel, M. (2021). All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. arXiv preprint arXiv:2109.04404 . Trinh, T. H. and Le, Q. V. (2018). A simple method for commonsense reasoning. arXiv preprint                                                                                                                                                               |\n| Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762 .                                                                                                                                                                                                                                                             |\n| Wang, N., Choi, J., Brand, D., Chen, C., and Gopalakrishnan, K. (2018). Training deep neural networks with 8-bit floating point numbers. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada , pages 7686-7695. |\n| Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F., and Liu, X. (2022). Out- lier suppression: Pushing the limit of low-bit transformer language models. arXiv preprint                                                                                                                                                                                                                                                    |\n| Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzm\u00e1n, F., Joulin, A., and Grave, E. (2020). CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference , pages 4003-4012, Marseille, France. European Language Resources Association.                                                                                                         |\n\n- Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. (2019). Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771 .\n- Wu, H., Judd, P., Zhang, X., Isaev, M., and Micikevicius, P. (2020). Integer quantization for deep learning inference: Principles and empirical evaluation. arXiv preprint arXiv:2004.09602 .\n- Yao, Z., Aminabadi, R. Y., Zhang, M., Wu, X., Li, C., and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 .\n- Yao, Z., Dong, Z., Zheng, Z., Gholami, A., Yu, J., Tan, E., Wang, L., Huang, Q., Wang, Y., Mahoney, M., et al. (2021). Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning , pages 11875-11886. PMLR.\n- Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M. (2019). Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS) , pages 36-39. IEEE.\n\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et al. (2022). Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 .\n\n- Zhang, D., Yang, J., Ye, D., and Hua, G. (2018). Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV) , pages 365-382.\n\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. (2022). Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 .\n\n- Zhang, W., Hou, L., Yin, Y., Shang, L., Chen, X., Jiang, X., and Liu, Q. (2020). Ternarybert: Distillation-aware ultra-low bit bert. In EMNLP .\n- Zhao, C., Hua, T., Shen, Y., Lou, Q., and Jin, H. (2021). Automatic mixed-precision quantization search of bert. arXiv preprint arXiv:2112.14938 .\n- Zhu, C., Han, S., Mao, H., and Dally, W. J. (2017). Trained ternary quantization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings . OpenReview.net.\n- Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision , pages 19-27.\n\n## Checklist\n\nThe checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or [N/A] . You are strongly encouraged to include a justification to your answer , either by referencing the appropriate section of your paper or providing a brief inline description. For example:\n\n- \u00b7 Did you include the license to the code and datasets? [Yes] See Section ?? .\n- \u00b7 Did you include the license to the code and datasets? [No] The code and the data are proprietary.\n- \u00b7 Did you include the license to the code and datasets? [N/A]\n\nPlease do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.\n\n- 1. For all authors...\n\n- (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]\n- (b) Did you describe the limitations of your work? [Yes] See the limitation section\n- (c) Did you discuss any potential negative societal impacts of your work?[Yes] See the Broader Impacts section\n- (d) Have you read the ethics review guidelines and ensured that your paper conforms to them?[Yes] Yes, we believe our work conforms to these guidelines.\n- 2. If you are including theoretical results...\n- (a) Did you state the full set of assumptions of all theoretical results? [N/A]\n- (b) Did you include complete proofs of all theoretical results? [N/A]\n- 3. If you ran experiments...\n- (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We will include our code in the supplemental material.\n- (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?[Yes] See the experimental setup section\n- (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Our experiments are deterministic for each model. Instead of running the same model multiple times, we run multiple models at different scales. We are unable to compute error bars for these experiments.\n- (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See the exper2imental setup section\n- 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n- (a) If your work uses existing assets, did you cite the creators? [Yes] See experimental setup section\n- (b) Did you mention the license of the assets? [No] The license is permissible for all the assets that we use. The individual licenses can easily be looked up.\n- (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] We only use existing datasets.\n- (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\n- (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\n- 5. If you used crowdsourcing or conducted research with human subjects...\n- (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n- (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n- (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## A Memory usage compared to 16-bit precision\n\nTable 3 compares the memory footprint of 16-bit inference and LLM.int8() for different open source models. We can see, that LLM.int8() allows to run the largest open source models OPT-175B and BLOOM-176B on a single node equipped with consumer-grade GPUs.\n\nTable 3: Different hardware setups and which methods can be run in 16-bit vs. 8-bit precision. We can see that our 8-bit method makes many models accessible that were not accessible before, in particular, OPT-175B/BLOOM.\n\n|                  |             |            | Largest Model that can be run   | Largest Model that can be run   |\n|------------------|-------------|------------|---------------------------------|---------------------------------|\n| Class            | Hardware    | GPU Memory | 8-bit                           | 16-bit                          |\n| Enterprise       | 8x A100     | 80 GB      | OPT-175B / BLOOM                | OPT-175B / BLOOM                |\n| Enterprise       | 8x A100     | 40 GB      | OPT-175B / BLOOM                | OPT-66B                         |\n| Academic server  | 8x RTX 3090 | 24 GB      | OPT-175B / BLOOM                | OPT-66B                         |\n| Academic desktop | 4x RTX 3090 | 24 GB      | OPT-66B                         | OPT-30B                         |\n| Paid Cloud       | Colab Pro   | 15 GB      | OPT-13B                         | GPT-J-6B                        |\n| Free Cloud       | Colab       | 12 GB      | T0/T5-11B                       | GPT-2 1.3B                      |\n\n## B Additional Related Work\n\nQuantization of Transformers with fewer than 1B Parameters Quantization of transformers has been focused on sub-billion parameter masked language model (MLMs), including BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019). Versions of 8-bit BERT/RoBERTa include Q8BERT (Zafrir et al., 2019), QBERT (Shen et al., 2020), product quantization with quantization noise (Fan et al., 2020), TernaryBERT (Zhang et al., 2020), and BinaryBERT (Bai et al., 2021). Work by Zhao et al. (2021) performs both quantization and pruning. All these models require either quantization-aware finetuning or post-training quantization to make the model usable in low-precision. In contrast with our methods, the model can be used directly without performance degradation.\n\nIf one views matrix multiplication as 1x1 convolution, vector-wise quantization is equivalent to channel-wise quantization for convolution combined with row quantization (Khudia et al., 2021). For matrix multiplication, this was used by Wu et al. (2020) for BERT-sized transformers (350M parameters), while we are the first to study vector-wise quantization for autoregressive and large-scale models. The only other work that we are aware of that quantizes transformers other than BERT is Chen et al. (2020), which uses post-training quantization with zeropoint quantization in the forward pass and zeropoint-row-wise quantization in the backward pass. However, this work is still for sub-billion parameter transformers. We compare with both zeropoint and row-wise quantization in our evaluations and do not require post-training quantization.\n\nLow-bitwidth and Convolutional Network Quantization Work that uses less than 8-bits for data types is usually for convolutional networks (CNNs) to reduce their memory footprint and increase inference speed for mobile devices while minimizing model degradation. Methods for different bit-widths have been studied: 1-bit methods (Courbariaux and Bengio, 2016; Rastegari et al., 2016; Courbariaux et al., 2015), 2 to 3-bit (Zhu et al., 2017; Choi et al., 2019), 4-bits (Li et al., 2019), more bits (Courbariaux et al., 2014), or a variable amount of bits (Gong et al., 2019). For additional related work, please see the survey of Qin et al. (2020). While we believe that lower than 8-bit width with some performance degradation is possible for billion-scale transformers, we focus on 8-bit transformers that do not degrade performance and that can benefit from commonly used GPUs that accelerates inference through Int8 tensor cores.\n\nAnother line of work that focuses on convolutional network quantization is to learn adjustments to the quantization procedure to improve quantization errors. For example, using Hessian information (Dong et al., 2019), step-size quantization (Esser et al., 2019), soft quantization (Gong et al., 2019), mixedprecision via linear programming optimization (Yao et al., 2021), and other learned quantization methods (Zhang et al., 2018; Gholami et al., 2021).\n\nTable 4: Summary statistics of outliers with a magnitude of at least 6 that occur in at least 25% of all layers and at least 6% of all sequence dimensions. We can see that the lower the C4 validation perplexity, the more outliers are present. Outliers are usually one-sided, and their quartiles with maximum range show that the outlier magnitude is 3-20x larger than the largest magnitude of other feature dimensions, which usually have a range of [-3.5, 3.5]. With increasing scale, outliers become more and more common in all layers of the transformer, and they occur in almost all sequence dimensions. A phase transition occurs at 6.7B parameters when the same outlier occurs in all layers in the same feature dimension for about 75% of all sequence dimensions (SDim). Despite only making up about 0.1% of all features, the outliers are essential for large softmax probabilities. The mean top-1 softmax probability shrinks by about 20% if outliers are removed. Because the outliers have mostly asymmetric distributions across the sequence dimension s , these outlier dimensions disrupt symmetric absmax quantization and favor asymmetric zeropoint quantization. This explains the results in our validation perplexity analysis. These observations appear to be universal as they occur for models trained in different software frameworks (fairseq, OpenAI, Tensorflow-mesh), and they occur in different inference frameworks (fairseq, Hugging Face Transformers). These outliers also appear robust to slight variations of the transformer architecture (rotary embeddings, embedding norm, residual scaling, different initializations).\n\n|       |       |        | Outliers   | Outliers   | Frequency   | Frequency   |                 | Top-1 softmax p   | Top-1 softmax p   |\n|-------|-------|--------|------------|------------|-------------|-------------|-----------------|-------------------|-------------------|\n| Model | PPL \u2193 | Params | Count      | 1-sided    | Layers      | SDims       | Quartiles       | w/ Outlier        | No Outlier        |\n| GPT2  | 33.5  | 117M   | 1          | 1          | 25%         | 6%          | (-8, -7, -6)    | 45%               | 19%               |\n| GPT2  | 26.0  | 345M   | 2          | 1          | 29%         | 18%         | (6, 7, 8)       | 45%               | 19%               |\n| FSEQ  | 25.7  | 125M   | 2          | 2          | 25%         | 22%         | (-40, -23, -11) | 32%               | 24%               |\n| GPT2  | 22.6  | 762M   | 2          | 0          | 31%         | 16%         | (-9, -6, 9)     | 41%               | 18%               |\n| GPT2  | 21.0  | 1.5B   | 2          | 1          | 41%         | 35%         | (-11, -9, -7)   | 41%               | 25%               |\n| FSEQ  | 15.9  | 1.3B   | 4          | 3          | 64%         | 47%         | (-33, -21, -11) | 39%               | 15%               |\n| FSEQ  | 14.4  | 2.7B   | 5          | 5          | 52%         | 18%         | (-25, -16, -9)  | 45%               | 13%               |\n| GPT-J | 13.8  | 6.0B   | 6          | 6          | 62%         | 28%         | (-21, -17, -14) | 55%               | 10%               |\n| FSEQ  | 13.3  | 6.7B   | 6          | 6          | 100%        | 75%         | (-44, -40, -35) | 35%               | 13%               |\n| FSEQ  | 12.5  | 13B    | 7          | 6          | 100%        | 73%         | (-63, -58, -45) | 37%               | 16%               |\n\n## C Detailed Outlier Feature Data\n\nTable 4 provides tabulated data from our outlier feature analysis. We provide the quartiles of the most common outlier in each transformer and the number of outliers that are one-sided, that is, which have asymmetric distributions which do not cross zero.\n\n## D Inference Speedups and Slowdowns\n\n## D.1 Matrix Multiplication benchmarks\n\nWhile our work focuses on memory efficiency to make models accessible, Int8 methods are also often used to accelerate inference. We find that the quantization and decomposition overhead is significant, and Int8 matrix multiplication itself only yields an advantage if the entire GPU is well saturated, which is only true for large matrix multiplication. This occurs only in LLMs with a model dimension of 4096 or larger.\n\nDetailed benchmarks of raw matrix multiplication and quantization overheads are seen in Table 5. We see that raw Int8 matrix multiplication in cuBLASLt begins to be two times faster than cuBLAS at a model size of 5140 (hidden size 20560). If inputs need to be quantized and outputs dequantized - a strict requirement if not the entire transformer is done in Int8 - then the speedups compared to 16-bit is reduced to 1.6x at a model size of 5140. Models with model size 2560 or smaller are slowed down. Adding mixed precision decomposition slows inference further so that only the 13B and 175B models have speedups.\n\nThese numbers could be improved significantly with optimized CUDA kernels for the mixed precision decomposition. However, we also see that existing custom CUDA kernels are much faster than when we use default PyTorch and NVIDIA-provided kernels for quantization which slow down all matrix multiplications except for a 175B model.\n\nTable 5: Inference speedups compared to 16-bit matrix multiplication for the first hidden layer in the feed-forward of differently sized GPT-3 transformers. The hidden dimension is 4x the model dimension. The 8-bit without overhead speedups assumes that no quantization or dequantization is performed. Numbers small than 1.0x represent slowdowns. Int8 matrix multiplication speeds up inference only for models with large model and hidden dimensions.\n\n| GPT-3 Size                      | Small   | Medium   | Large   | XL    | 2.7B   | 6.7B   | 13B   | 175B   |\n|---------------------------------|---------|----------|---------|-------|--------|--------|-------|--------|\n| Model dimension                 | 768     | 1024     | 1536    | 2048  | 2560   | 4096   | 5140  | 12288  |\n| FP16-bit baseline               | 1.00x   | 1.00x    | 1.00x   | 1.00x | 1.00x  | 1.00x  | 1.00x | 1.00x  |\n| Int8 without overhead           | 0.99x   | 1.08x    | 1.43x   | 1.61x | 1.63x  | 1.67x  | 2.13x | 2.29x  |\n| Absmax PyTorch+NVIDIA           | 0.25x   | 0.24x    | 0.36x   | 0.45x | 0.53x  | 0.70x  | 0.96x | 1.50x  |\n| Vector-wise PyTorch+NVIDIA      | 0.21x   | 0.22x    | 0.33x   | 0.41x | 0.50x  | 0.65x  | 0.91x | 1.50x  |\n| Vector-wise                     | 0.43x   | 0.49x    | 0.74x   | 0.91x | 0.94x  | 1.18x  | 1.59x | 2.00x  |\n| LLM.int8() (vector-wise+decomp) | 0.14x   | 0.20x    | 0.36x   | 0.51x | 0.64x  | 0.86x  | 1.22x | 1.81x  |\n\n## D.2 End-to-end benchmarks\n\nBesides matrix multiplication benchmarks, we also test the end-to-end inference speed of BLOOM176B in Hugging Face. Hugging Face uses an optimized implementation with cached attention values. Since this type of inference is distributed and, as such, communication dependent, we expect the overall speedup and slowdown due to Int8 inference to be smaller since a large part of the overall inference runtime is the fixed communication overhead.\n\nWe benchmark vs. 16-bit and try settings that use a larger batch size or fewer GPUs in the case of Int8 inference, since we can fit the larger model on fewer devices. We can see results for our benchmark in Table 6. Overall Int8 inference is slightly slower but close to the millisecond latency per token compared to 16-bit inference.\n\nTable 6: Ablation study on the number of GPUs used to run several types of inferences of BLOOM176B model. We compare the number of GPUs used by our quantized BLOOM-176B model together with the native BLOOM-176B model. We also report the per-token generation speed in milliseconds for different batch sizes. We use our method integrated into transformers(Wolf et al., 2019) powered by accelerate library from HuggingFace to deal with multi-GPU inference. Our method reaches a similar performance to the native model by fitting into fewer GPUs than the native model.\n\n| Batch Size        | Hardware    |   1 |   8 |    32 |\n|-------------------|-------------|-----|-----|-------|\n| bfloat16 baseline | 8xA100 80GB | 239 |  32 |  9.94 |\n| LLM.int8()        | 8xA100 80GB | 253 |  34 | 10.44 |\n| LLM.int8()        | 4xA100 80GB | 246 |  33 |  9.4  |\n| LLM.int8()        | 3xA100 80GB | 247 |  33 |  9.11 |\n\n## E Training Results\n\nWe test Int8 training on a variety of training settings and compare to 32-bit baselines. We test separate settings for running the transformer with 8-bit feed-forward networks with and without 8-bit linear projections in the attention layer, as well at the attention iteself in 8-bit and compare against 32-bit performance. We test two tasks (1) language modeling on part of the RoBERTa corpus including Books (Zhu et al., 2015), CC-News (Nagel, 2016), OpenWebText (Gokaslan and Cohen, 2019), and CC-Stories (Trinh and Le, 2018); and (2) neural machine translation (NMT) (Ott et al., 2018) on WMT14+WMT16 (Mach\u00e1\u02c7cek and Bojar, 2014; Sennrich et al., 2016).\n\nThe results are shown in Table 7 and Table 8. We can see that for training, using the attention linear projections with Int8 data types and vector-wise quantization leads to degradation for NMT and for 1.1B language model but not for 209M language modeling. The results improve slightly if mixed-precision decomposition is used but is not sufficient to recover full performance in most cases. These suggests that training with 8-bit FFN layers is straightforward while other layers require\n\nadditional techniques or different data types than Int8 to do 8-bit training at scale without performance degradation.\n\nTable 7: Initial results on small and large-scale language modeling. Doing attention in 8-bit severely degrades performance and performance cannot fully recovered with mixed-precision decomposition. While small-scale language models is close to baseline performance for both 8-bit FFN and 8-bit linear projects in the attention layers performance degrades at the large scale.\n\nTable 9 compares with different previous 8-bit methods for finetuning and shows that vector-wise quantization improves on other methods. Table 10 shows the performance of FFN and/or linear attention projections in 8-bit as well as improvements if mixed-precision decomposition is used. We find that 8-bit FFN layers lead to no degradation while 8-bit attention linear projections lead to degradation if not combined with mixed-precision decomposition where at least the top 2% magnitude dimensions are computed in 16-bit instead of 8-bit. These results highlight the critical role of mixed-precision decomposition for finetuning if one wants to not degrade performance.\n\n|        | Is 8-bit     | Is 8-bit       | Is 8-bit     |        |       |\n|--------|--------------|----------------|--------------|--------|-------|\n| Params | FFN          | Linear         | Attention    | Decomp | PPL   |\n| 209M   |              |                |              | 0%     | 16.74 |\n| 209M   | glyph[check] |                |              | 0%     | 16.77 |\n| 209M   | glyph[check] | glyph[check] . |              | 0%     | 16.83 |\n| 209M   | glyph[check] | glyph[check]   |              | 2%     | 16.78 |\n| 209M   | glyph[check] | glyph[check]   |              | 5%     | 16.77 |\n| 209M   | glyph[check] | glyph[check]   |              | 10%    | 16.80 |\n| 209M   | glyph[check] | glyph[check]   | glyph[check] | 2%     | 24.33 |\n| 209M   | glyph[check] | glyph[check]   | glyph[check] | 5%     | 20.00 |\n| 209M   | glyph[check] | glyph[check]   | glyph[check] | 10%    | 19.00 |\n| 1.1B   |              |                |              | 0%     | 9.99  |\n| 1.1B   | glyph[check] |                |              | 0%     | 9.93  |\n| 1.1B   | glyph[check] | glyph[check]   |              | 0%     | 10.52 |\n| 1.1B   | glyph[check] | glyph[check]   |              | 1%     | 10.41 |\n\n## F Fine-tuning Results\n\nWe also test 8-bit finetuning on RoBERTa-large finetuned on GLUE. We run two different setups: (1) we compare with other Int8 methods, and (2) we compare degradation of finetuning with 8-bit FFN layers as well as 8-bit attention projection layers comparel to 32-bit. We finetune with 5 random seeds and report median performance.\n\nTable 8: Neural machine translation results for 8-bit FFN and linear attention layers for WMT14+16. Decomp indicates the percentage that is computed in 16-bit instead of 8-bit. The BLEU score is the median of three random seeds.\n\n| Is 8-bit     | Is 8-bit     |        |           |\n|--------------|--------------|--------|-----------|\n| FFN          | Linear       | Decomp | BLEU      |\n| glyph[check] |              | 0% 0%  | 28.9 28.8 |\n| glyph[check] | glyph[check] | 0%     | unstable  |\n| glyph[check] | glyph[check] | 2%     | 28.0      |\n| glyph[check] | glyph[check] | 5%     | 27.6      |\n| glyph[check] | glyph[check] | 10%    | 27.5      |\n\nTable 9: GLUE finetuning results for quantization methods for the feedforward layer in 8-bit while the rest is in 16-bit. No mixed-precision decomposition is used. We can see that vector-wise quantization improve upon the baselines.Table 10: Breakdown for 8-bit feedforward network (FFN) and linear attention layers for GLUE. Scores are median of 5 random seeds. Decomp indicates the percentage that is decomposed into 16-bit matrix multplication. Compared to inference, fine-tuning appears to need a higher decomp percentage if the linear attention layers are also converted to 8-bit.\n\n| Method                       |   MNLI |   QNLI |   QQP |   RTE |   SST-2 |   MRPC |   CoLA |   STS-B |   Mean |\n|------------------------------|--------|--------|-------|-------|---------|--------|--------|---------|--------|\n| 32-bit Baseline              |   90.4 |   94.9 |  92.2 |  84.5 |    96.4 |   90.1 |   67.4 |    93   |  88.61 |\n| 32-bit Replication           |   90.3 |   94.8 |  92.3 |  85.4 |    96.6 |   90.4 |   68.8 |    92   |  88.83 |\n| Q-BERT (Shen et al., 2020)   |   87.8 |   93   |  90.6 |  84.7 |    94.8 |   88.2 |   65.1 |    91.1 |  86.91 |\n| Q8BERT (Zafrir et al., 2019) |   85.6 |   93   |  90.1 |  84.8 |    94.7 |   89.7 |   65   |    91.1 |  86.75 |\n| PSQ (Chen et al., 2020)      |   89.9 |   94.5 |  92   |  86.8 |    96.2 |   90.4 |   67.5 |    91.9 |  88.65 |\n| Vector-wise                  |   90.2 |   94.7 |  92.3 |  85.4 |    96.4 |   91   |   68.6 |    91.9 |  88.81 |\n\n| FFN          | Linear       | Decomp   |   MNLI |   QNLI |   QQP |   RTE |   SST-2 |   MRPC |   CoLA |   STS-B |   MEAN |\n|--------------|--------------|----------|--------|--------|-------|-------|---------|--------|--------|---------|--------|\n|              |              | 0%       |   90.4 |   94.9 |  92.2 |  84.5 |    96.4 |   90.1 |   67.4 |    93   |   88.6 |\n| glyph[check] |              | 0%       |   90.2 |   94.7 |  92.3 |  85.4 |    96.4 |   91   |   68.6 |    91.9 |   88.8 |\n| glyph[check] | glyph[check] | 0%       |   90.2 |   94.4 |  92.2 |  84.1 |    96.2 |   89.7 |   63.6 |    91.6 |   87.7 |\n| glyph[check] | glyph[check] | 1%       |   90   |   94.6 |  92.2 |  83   |    96.2 |   89.7 |   65.8 |    91.8 |   87.9 |\n| glyph[check] | glyph[check] | 2%       |   90   |   94.5 |  92.2 |  85.9 |    96.7 |   90.4 |   68   |    91.9 |   88.7 |\n| glyph[check] | glyph[check] | 3%       |   90   |   94.6 |  92.2 |  86.3 |    96.4 |   90.2 |   68.3 |    91.8 |   88.7 |", "title": "LLMint8_8-bit_Matrix_Multiplication_for_Transformers_at_Scale", "expert_id": "67847c5880957f028e351613", "link": "https://arxiv.org/pdf/2208.07339", "published_at": "2022-08-15 17:08:50"}, {"_id": "67847ef9342d0974a0ada2d0", "content": "## ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\n\n## Jon Saad-Falcon\n\nStanford University \u2217 jonsaadfalcon@stanford.edu\n\n## Christopher Potts\n\nStanford University cgpotts@stanford.edu\n\n## Abstract\n\nEvaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System , for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.\n\n## 1 Introduction\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer support (Petroni et al., 2021; Wang et al., 2019). Typically, a RAG system consists of a retriever and a downstream language model (LM). Given a user question, the retriever finds relevant passages from a corpus and the LM uses these passages to generate a response. This formulation admits a multitude of choices: what retrieval model to use, how to divide the documents into retrieval chunks, and how to prompt or finetune the LM to use the retrieved information, to name only a few of the simplest design decisions.\n\n## Omar Khattab\n\nStanford University okhattab@stanford.edu\n\n## Matei Zaharia\n\nDatabricks and UC Berkeley matei@databricks.com\n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes, and cost/latency budgets. To tune their own RAG systems, practitioners traditionally need hand annotations for test questions, passages to retrieve (to assess the retriever), and responses to generate, labeled specifically for their target domain. Alternatively, they may evaluate different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs.\n\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al., 2023). For instance, the open-source RAGAS framework (James and Es, 2023) prompts an LM for evaluating the relevance of retrieved information and the faithfulness and accuracy of generated responses. Unfortunately, such strategies currently rely for evaluation on a fixed set of heuristically hand-written prompts, offering little adaptability to various evaluation contexts and no guarantees about quality.\n\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the A utomated R AG E valuation S ystem. ARES is the first automated RAG evaluation system to generate tailored LLM judges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and accuracy compared to existing approaches like RAGAS. Furthermore, unlike existing RAG evaluation systems, ARES provides confidence intervals for its scoring by leveraging prediction-powered inference (PPI; Angelopoulos et al. 2023). Given a corpus of documents and a RAG system, ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question). A good\n\nRAG system finds relevant contexts and generates answers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring. ARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more), which are used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question-answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks (context relevance, answer faithfulness, and answer relevance). These judges are lightweight models fine-tuned against a contrastive learning objective. Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring. PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set , which is composed of approximately 150 annotated datapoints or more that designate both positive and negative examples for context relevance, answer faithfulness, and answer relevance.\n\nWe conduct extensive empirical evaluations, demonstrating that ARES accurately scores RAG systems across the six knowledge-intensive datasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS by 59.3 and 14.4 percentage points on average across context relevance and answer relevance evaluation accuracy, respectively. Additionally, ARES accurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al., 2022), predicting within 2.5 percentage points of the ground truth average for answer hallucinations. Compared to annotation-based evaluation methods, ARES is substantially more accurate and efficient, requiring 78% less annotations than the baseline approach. We also find that ARES consistently distinguishes competitive RAG systems that are only a few points apart in ground-truth metrics. This precision enables ARES to guide the develop-\n\nment and comparison of competitive approaches and configurations.\n\nWe make the ARES code and datasets publicly available on Github.\n\n## 2 Related Work\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a common strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval, RAG helps LM systems gather domain-specific knowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and offer a degree of transparency or interpretability via citing sources (Mialon et al., 2023).\n\nMultiple LLM-based evaluation techniques have emerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it is difficult to build a traditional benchmark dataset from scratch. Early attempts at this use LLMs out of the box, as in MT-Bench and Chatbot Arena (Zheng et al., 2023). AutoCalibrate (Liu et al., 2023b) seeks to align an LLM-judge with human preferences, leveraging a self-refinement prompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical guarantees for the accuracy of its predictions. Other work has used LLM prompting to evaluate system quality across natural language generation tasks, such as translation, summarization, and dialogue (Kocmi and Federmann, 2023; Fu et al., 2023; Liu et al., 2023a; Wang et al., 2023).\n\nIn the context of knowledge-intensive NLP tasks, LLMs have been explored for assessing attribution and factuality in LLMs (Min et al., 2023; Gekhman et al., 2023; Yue et al., 2023). New guidelines like LongEval (Krishna et al., 2023) and datasets like Hagrid and ALCE (Kamalloo et al., 2023; Gao et al., 2023) provide resources for analyzing knowledge-intensive LLM pipelines.\n\nThe two most-closely related projects to ARES are EXAM (Sander and Dietz, 2021) and RAGAS (James and Es, 2023). To evaluate RAG systems, the EXAM metric estimates how many exam questions a reader (simulated as a QA system) can answer correctly based on the generated response. This requires a set of queries with several associated sub-questions each, which adds a burden that ARES does not bring. RAGAS is based on a handful of heuristic hand-written prompts. These offer little adaptability to new RAG evaluation set-\n\ntings (e.g., new corpora) and, as we show in our evaluation, substantially underperform ARES.\n\n## 3 ARES\n\nARES proceeds in three stages (Figure 1). There are three required inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints (or more), and few-shot examples of in-domain queries and answers (five or more examples), which are used for prompting LLMs in synthetic data generation. With our inputs prepared, we begin by generating synthetic queries (and their answers) from the passages in the target corpus. We then use these query-passage-answer triples to train LLM judges. Subsequently, we apply these judges to any RAG system, scoring a sample of its in-domain query-document-answer triples, and use prediction-powered inference (PPI) with our human preference validation set to estimate a confidence interval for the quality of each RAG system.\n\n## 3.1 LLMGeneration of Synthetic Dataset\n\nWe generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query-passage-answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers). For generation, the LLM uses our input set of few-shot examples with in-domain passages mapped to in-domain queries and answers; the model then generates a synthetic question and answer from a given in-domain passage, allowing us to create both positive and negative training examples. We include example prompts for generating synthetic queries and answers in A.6.\n\nFor creating our synthetic data, we primarily use on FLAN-T5 XXL (discussed in subsection 4.1). ARES works well with this model (see section 5) but our system can ultimately use another highquality model for generating synthetic queries and answers. We then filter out low-quality queries by testing if a given query can retrieve its original passage as the top result using its retriever. This filtering approach has been used in previous work to isolate high-quality synthetic queries (Dai et al., 2022; Saad-Falcon et al., 2023).\n\nTo generate negatives for fine-tuning our LLM judges, we rely on two novel strategies, generating the same number of negatives with each strategy:\n\n- 1. Weak Negative Generation : For context rel-\n\nevance negatives, we randomly sample indomain passages unrelated to a given synthetic query. For answer faithfulness and answer relevance negatives, we randomly sample synthetically-generated answers from other passages, which were created using FLAN-T5 XXL.\n\n- 2. Strong Negative Generation : For context relevance negatives, we randomly sample indomain passages from the same document as the gold passage. For datasets in which multiple passages are not available for the same document, we use BM25 to retrieve the top10 passages similar to the passage and sample from them for our context relevance strong negatives. For answer faithfulness and answer relevance negatives, we prompt FLANT5 XXL (subsection 4.1) to generate a contradictory answer using the few-shot prompt in subsection A.5.\n\nIn total, the number of negatives generated equals the number of positives generated for evaluating context relevance and answer relevance.\n\n## 3.2 Preparing LLM Judges\n\nTo prepare our RAG evaluation judges, we use our synthetic dataset to fine-tune DeBERTa-v3Large judges (discussed in subsection 4.1) to evaluate three different capabilities (Chen et al., 2023; James and Es, 2023):\n\n- 1. Context Relevance : Is the passage returned relevant for answering the given query?\n- 2. Answer Faithfulness : Is the answer generated faithful to the retrieved passage, or does it contain hallucinated or extrapolated statements beyond the passage?\n- 3. Answer Relevance : Is the answer generated relevant given the query and retrieved passage?\n\nFor each metric, a separate LLM with a binary classifier head is fine-tuned to classify positive and negative examples. For each concatenated querydocument-answer, a single LLM judge must classify the triple as positive or negative for that judge's metric. To fine-tune these judges, we use our human preference validation set to evaluate model improvement after each epoch, stopping when we have three epochs with no improvement in loss (see subsection A.1 for more information).\n\nFigure 1: Overview of ARES : As inputs, the ARES pipeline requires an in-domain passage set, a human preference validation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or more), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation, we first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a constrastive learning framework, we fine-tune an LLM to classify query-passage-answer triples in three different criteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n\n<!-- image -->\n\n## 3.3 Ranking RAG Systems with Confidence Intervals\n\nOnce we have prepared our LLM judges, we need to use them to score and rank the competing RAG systems. To do this, ARES samples the in-domain query-document-answer triples produced by each RAG approach, and the judges label each triple, predicting their context relevance, answer faithfulness, and answer relevance. By averaging the individual predicted labels for each in-domain triple, we calculate the RAG system performance across each of the three metrics.\n\nIn principle, we could simply report these average scores as quality metrics for each RAG system. However, these scores reflect entirely unlabeled data with predictions from a synthetically-trained LLM judge, and hence they may not be entirely accurate. As an extreme alternative, we could use just the small human preference validation set discussed previously for evaluation, reporting the extent to which each RAG system agrees with (or deviates from) the human annotations. However, an annotation-based evaluation approach would require labeling substantially more generative outputs from each RAG systems separately, which can be costly both in terms of time and financing.\n\nTo combine the benefits of both, and hence boost the precision of the evaluation, ARES uses prediction-powered inference (PPI; Angelopoulos et al. 2023) to predict the system scores. PPI is a recent statistical method that provides tighter confidence intervals on a small set of annotated datapoints (i.e., our validation set) by leveraging predictions on a much larger set of non-annotated datapoints. PPI can leverage both the labeled dat-\n\napoints and the ARES judge predictions on the non-annotated datapoints to construct confidence intervals for our RAG system's performance.\n\nTo do this, PPI uses the LLM judges on the human preference validation set to learn a rectifier function for constructing a confidence set of the ML model's performance, using each ML prediction in the larger non-annotated dataset. The confidence set can then be used to create a tighter confidence interval for the performance of the evaluated RAG system (e.g. its context relevance, answer faithfulness, or answer relevance accuracy individually) compared to simply using annotated outputs from the evaluated RAG system. By bolstering the human preference validation set with the much larger set of datapoints with ML predictions, PPI can develop reliable confidence intervals for ML model performance that beat previous classical inference approaches.\n\nThe PPI rectifier function allows us to estimate the errors of the LLM judge and generate confidence bounds for the success and failure rates of the RAG system, estimating context relevance, answer faithfulness, and answer relevance performance. Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval.\n\nWith the accuracy confidence interval for each component of the RAG, we find the midpoint of each confidence interval and use the midpoints to rank the RAG systems. With our ranking, we can compare different RAG systems, as well as different configurations of the same RAG system, to find the best-performing approach for a given domain.\n\n## 4 Experiments\n\n## 4.1 Models\n\nFor our fine-tuned judges, ARES relies on generating cheap but quality synthetic queries and answers using LLMs. For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge. Our fine-tuned LLM judges allow us to rank RAG systems without relying on external APIs, solely using few-shot prompts and deployable LLMs on commercial GPUs.\n\nFor our in-context learning baseline, we use OpenAI's gpt-3.5-turbo-16k , version 10/23, (Brown et al., 2020) in a zero/few-shot setting. For similarity search over in-domain passages, we use FAISS IndexFlatL2 for indexing (Johnson et al., 2019) and OpenAI's text-embedding-ada-002 for generating embeddings. We use simlarity search over in-domain passages to filter our synthetic queries that cannot retrieve the passage from which they were generated. We use version 0.0.18 of RAGAS in our experiments (James and Es, 2023).\n\n## 4.2 Datasets\n\nOur core experimental goal is to provide a rich picture of where ARES can be applied effectively. To test across multiple types of queries, documents, and answers, we selected all the datasets from the widely-used KILT and SuperGLUE benchmarks for which RAG is appropriate.\n\nFrom KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW) (Kwiatkowski et al., 2019; Yang et al., 2018; Akhtar et al., 2023; Dinan et al., 2018). Each dataset uses Wikipedia passages but the queries and answers offer a range of applications. Both NQ and HotpotQA feature direct questions and expect short answers, but NQ uses single passages for reasoning while HotpotQA requires multiple passages for reasoning. Furthermore, FEVER focuses on fact-verification, determining if a passage supports or refutes a given statement, and expects an output of 'SUPPORTS' or 'REFUTES'. WoWseeks to evaluate dialogue agents by mapping user dialogue to relevant Wikipedia passages before a chatbot generates a paragraph-length chat response incorporating passage knowledge.\n\nFrom SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD (Khashabi et al., 2018; Zhang et al., 2018). MultiRC focuses on direct questions for seven different domains (News,\n\nWikipedia articles, articles on society/law/justice, articles on history/anthropology, elementary school science textbooks, 9/11 reports, and fiction). ReCoRD focuses on determining the placeholder entity in a statement, focusing on news articles from CNN and the Daily Mail. For MultiRC and ReCoRD, we create open-domain versions of their tasks. For MultiRC, we perform retrieval over its seven sets of domain passages. For ReCoRD, we perform retrieval over its news article passages.\n\nThe efficacy of ARES relies on its ability to rank different RAG systems while only using a human preference validation set and domain-targeted LLM judges. To test the limits of ARES, we need to simulate the existence of many RAG systems that are separated by small accuracy margins on our evaluation metrics. For this, we create systems using artificial query-passage-answer triples, in which we empirically know the positive and negative examples of the mock RAG system. We generate these mock splits of the given datasets by selecting (1) The positive and negative query-passage matches for context relevance, and (2) the positive and negative query-passage-answer matches for answer relevance. We include positive and negative examples from our evaluation sets in Table 7.\n\nFor our positive triples, we can simply use the KILT and SuperGLUE examples without any alteration. For gathering negative query-passage pairs and query-passage-answer triples, we randomly sample passages and answers from either: the same Wikipedia document or an entirely random Wikipedia document. This sampling allows us to artificially create mock RAG systems for testing ARES. By sampling both related and unrelated documents/answers, we hope to better gauge the efficacy of ARES in judging RAG outputs.\n\nWe do not evaluate answer faithfulness for KILT and SuperGLUE datasets since we do not have human-annotated hallucinated answers to use for evaluation. However, we do test the ARES framework on real attribution datasets in Section 5.2.\n\nUsing the validation subsets for each KILT and SuperGLUE dataset, we create nine different dataset splits, ranging from 70% success rate to 90% success rate for each of the evaluated RAG criteria; each dataset is separated by 2.5% accuracy points (e.g. 70.0%, 72.5%, 75.0%, ... , 90.0%). Each split also represents a different mock RAG system. Since we know the success percentages of each dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\ntest ARES success at both scoring and ranking the mock RAG systems appropriately across the three evaluation criteria.\n\n## 4.3 Metrics\n\nTo calculate the correlation between the correct ranking and the ARES ranking, we use the Kendall rank correlation coefficient or Kendall's \u03c4 :\n\n\u03c4 = (# of concordant pairs ) -(# of discordant pairs ) # of pairs total\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the sequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal values in the ranking where the earlier value in the sequence is greater than or equal to the later value in the sequence. A Kendall's \u03c4 greater than 0.9 is considered successful but it ranges from 0.0 to 1.0.\n\nIn development, researchers and engineers will be comparing different RAG configurations through individual pairwise comparisons of model choices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a variety of performance gaps between RAG systems. Kendall's \u03c4 is explicitly designed for measuring the accuracy of such pairwise comparisons, calculating the correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking. Thus, it is a popular and widespread metric used in information retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we believe Kendall's tau and prediction accuracy provide meaningful metrics for testing the efficacy of ARES as a RAG evaluation system.\n\n## 5 Results & Analysis\n\n## 5.1 ARES Ranking\n\nTable 1 summarizes our main evaluation of ARES (with DeBERTa-v3-Large as the pretrained basis for the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( gpt-3.5-turbo-16k ). For the few-shot GPT-3.5 judge, we provide few-shot examples for guiding predictions; the prompts are included in Appendices A.2, A.3, and A.4. For both ARES and the GPT-3.5 judge baseline, we augment the LLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions and produce confidence intervals.\n\nAcross almost all settings across the datasets from KILT and SuperGLUE, ARES provides a more accurate ranking of RAG systems than RAGAS. ARES averages a Kendall's \u03c4 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS . Additionally, the LLMjudge is substantially more accurate than RAGAS at predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS . Overall, ARES provides a more accurate system for automatically evaluating RAG configurations than RAGAS by leveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster model predictions.\n\nAs an additional comparison, we also include the Kendall's \u03c4 for RAG ranking with the ARES LLM judge without PPI; for all datasets tested, PPI improved the ranking prediction accuracy of the fine-tuned LLM judge. Furthermore, we included a sampled annotations configuration, in which we sampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all these annotations, the Kendall's \u03c4 for ARES is 0.08 higher on average, across both context and answer relevance, compared to sampled annotations, despite using 78% less annotations. In sum, ARES proves significantly more data-efficient with human annotations while being more accurate at scoring than standard sampled annotation methods.\n\nCompared to the GPT-3.5 judge, ARES provides a more accurate ranking of the RAG systems than the GPT-3.5 judge, averaging a Kendall's tau 0.06 higher over both context relevance and answer relevance. Between the judge configurations, the finetuned LLM judge of ARES can more precisely distinguish between RAG systems and guide configuration decisions surrounding document splitting, retriever selection, and generative LLM choice. However, while the fine-tuned LLM judge had a higher Kendall's tau on average, the GPT-3.5 judge is more readily deployable and does not require any additional fine-tuning. The GPT-3.5 judge does come with its own querying costs, which can vary based on the date of querying as well as the total tokens used in evaluation.\n\nWe also wanted to better understand the importance of human annotations for ARES. To this end, we conducted two sets of experiments. First, we\n\nTable 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge : For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets. The Kendall's tau for ARES was 0.065 higher on average for scoring context relevance and 0.132 higher on average for scoring answer relevance than RAGAS. Additionally, we include the Kendall's taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n|                                                                                                | ARES Ranking of Pseudo RAG Systems                                      | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems                                      | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   | ARES Ranking of Pseudo RAG Systems   |\n|------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|-------------------------------------------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|\n|                                                                                                | NQ                                                                      | NQ                                   | HotpotQA                             | HotpotQA                             | WoW                                                                     | WoW                                  | FEVER                                | FEVER                                | MultiRC                              | MultiRC                              | ReCoRD                               | ReCoRD                               |\n|                                                                                                | C.R                                                                     | A.R.                                 | C.R                                  | A.R.                                 | C.R                                                                     | A.R.                                 | C.R                                  | A.R.                                 | C.R                                  | A.R.                                 | C.R                                  | A.R.                                 |\n| Kendall's Tau for Sampled Annotations                                                          | 0.83                                                                    | 0.89                                 | 0.78                                 | 0.78                                 | 0.78                                                                    | 0.83                                 | 0.89                                 | 0.89                                 | 0.83                                 | 0.83                                 | 0.72                                 | 0.94                                 |\n| Kendall's Tau for RAGAS                                                                        | 0.89                                                                    | 0.89                                 | 0.94                                 | 0.89                                 | 0.94                                                                    | 0.94                                 | 0.72                                 | 0.61                                 | 0.83                                 | 0.94                                 | 0.89                                 | 0.44                                 |\n| Kendall's Tau for GPT-3.5 Judge                                                                | 0.89                                                                    | 0.94                                 | 0.67                                 | 0.94                                 | 0.94                                                                    | 0.89                                 | 0.78                                 | 0.78                                 | 0.83                                 | 0.89                                 | 0.83                                 | 0.94                                 |\n| Kendall's Tau for ARES LLM Judge                                                               | 0.89                                                                    | 1.0                                  | 0.89                                 | 0.94                                 | 0.94                                                                    | 1.0                                  | 0.83                                 | 0.72                                 | 0.94                                 | 0.83                                 | 0.78                                 | 0.83                                 |\n| Kendall's Tau for ARES                                                                         | 0.94                                                                    | 1.0                                  | 0.94                                 | 0.94                                 | 1.0                                                                     | 1.0                                  | 0.89                                 | 0.78                                 | 0.94                                 | 0.89                                 | 0.83                                 | 0.89                                 |\n| RAGAS Accuracy                                                                                 |                                                                         |                                      |                                      |                                      | 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8% |                                      |                                      |                                      |                                      |                                      |                                      |                                      |\n| GPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8% |                                                                         |                                      |                                      |                                      |                                                                         |                                      |                                      |                                      |                                      |                                      |                                      |                                      |\n| ARES Accuracy                                                                                  | 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3% |                                      |                                      |                                      |                                                                         |                                      |                                      |                                      |                                      |                                      |                                      |                                      |\n\nused ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3). Second, we explored whether GPT-4 generations could replace human annotations entirely, finding that GPT-4 is less good than humans in this role, though the idea arguably has promise (Table 4).\n\n## 5.2 ARES Performance on AIS\n\nTable 2: ARES Results on the AIS benchmark\n\n|                                 | WoW   | CNN / DM   |\n|---------------------------------|-------|------------|\n| ARES Split Prediction           | 0.478 | 0.835      |\n| Correct Positive/Negative Split | 0.458 | 0.859      |\n| ARES Judge Accuracy             | 62.5% | 84.0%      |\n| Evaluation Set Size             | 707   | 510        |\n| Human Preference Data Size      | 200   | 200        |\n\nTo evaluate whether ARES can effectively gauge answer faithfulness in real RAG systems, we tested ARES on the AIS attribution benchmark (Rashkin et al., 2022). In AIS, we selected the Wizards of Wikipedia (WoW) and CNN/DM datasets; the\n\nother benchmark datasets involve either table reasoning (ToTTo) or focus on passage summarization (QRECC) so we excluded them. In WoW and CNN/DM, each evaluation example includes a query, a retrieved passage, and a generated answer (which is either faithful or non-attributed to the retrieved passage).\n\nTable 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores. Furthermore, for scoring each system, we only use 200 annotated datapoints for our human preference validation set. Our results on AIS demonstrate the ability of ARES to reliably distinguish faithful and hallucinated answers in realworld RAG systems.\n\n## 5.3 ARES Ranking of Existing RAG Systems\n\nWe also wanted to evaluate whether ARES can score and rank existing RAG systems across both context relevance and answer relevance. For evaluation, we selected the NQ, WoW, and FEVER datasets from KILT. We consider the answer gen-\n\nerations to be correct if they contained the KILT answer in their output. For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2 (Santhanam et al., 2022)) and three different generative LLMs (MPT-7b-Instruct (Team, 2023), GPT-3.5-Turbo, and GPT-4). Additionally, we include the Facebook RAG model (Lewis et al., 2020), which uses a DPR retriever (Karpukhin et al., 2020) and BART sequence-tosequence model (Lewis et al., 2019). During retrieval, each RAG system only retrieves one passage to assist generation.\n\nIn Table 5, we found that ARES can reliably score and rank RAG systems in real-world applications, averaging a Kendall's tau of 0.91 for context relevance and 0.97 for answer relevance. Compared to RAGAS, ARES is 0.16 higher for context relevance and 0.15 higher for answer relevance, on average. ARES also provided accurate confidence bounds for its predictions, capturing the ground truth average outcomes for context relevance and answer relevance more than 95% of the time; on average, the PPI confidence intervals were 7.4 points wide for context relevance and 6.1 points wide for answer relevance (see Figure 2 and Figure 3 for ARES vs. RAGAS). Among the models tested, the best performing retriever was ColBERTv2 while the best performing generative LLM was GPT-4.\n\n## 5.4 Strengths and Limits of Cross-Domain Applications\n\nThe generalizability of the LLM judge used in ARES is critical for deploying our framework in specialized domains, particularly domains where in-domain queries, documents, and answers are difficult to gather. Therefore, we wanted to test how the LLM judges used in ARES would be affected by three domain shifts: change in query type from training to test (e.g. NQ to FEVER), change in document type from training to test (e.g. NQ to MultiRC), and change in both query and document type (e.g. NQ to ReCoRD).\n\nIn Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in crossdomain applications. Across all settings, we found that LLM judges in ARES had strong generalizability, even when only using 300 datapoints in our human preference validation set for PPI. Furthermore, we found that even when the LLM judge's accuracy suffered in cross-domain applications, PPI helped mitigate the loss in accuracy and still allow\n\nARES to be successful. Additional examples for PPI also continued to boost cross-domain ARES performance in subsequent tests.\n\nWhile LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages (e.g. English to Spanish, German, and other languages), switching from text to code (e.g. questions + passages to coding functions + documentation), and switching from retrieving text to extraction of entities, webpages, or citations.\n\nTo test cross-lingual transfer, we used the XGLUE datasets (Liang et al., 2020); a LLM judge fine-tuned on NQ achieved a Kendall's tau of 0.33 over both context relevance and answer relevance scoring for XGLUE. To test text-to-code, we used CodeSearchNet (Husain et al., 2019); an LLM judge fine-tuned on NQ achieved a Kendall's tau of 0.28 over both context relevance and answer relevance scoring for CodeSearchNet. To test extraction task generalizability, we used T-Rex from KILT (Elsahar et al., 2018; Petroni et al., 2021); an LLM judge fine-tuned on NQ achieved a Kendall's tau of 0.38 over both context relevance and answer relevance scoring for T-Rex. Each cross-domain shift requires in-domain passages and few-shot query examples for reconfiguring ARES judges.\n\n## 6 Conclusion\n\nIn this work, we present ARES, a novel automated evaluation framework for retrieval-augmented generation (RAG). ARES offers a novel training pipeline for fine-tuning lightweight LLM judges on synthetically generated queries and answers. ARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires only minimal human annotations. For the eight different datasets in KILT, SuperGLUE, and AIS requiring RAG-based solutions, we found that ARES can accurately score and rank RAG systems based on context relevance, answer faithfulness, and answer relevance scores, beating the existing RAGAS automated evaluation framework.\n\nARES is a flexible framework, and there may be variants of it that are even more powerful than the ones we explored here. Avenues to explore include GPT-4 as a replacement for human labeling (Table 4), more robust techniques for the synthetic datasets used in fine-tuning LLM judges, utilizing\n\nlogits in LLM judge prediction to improve PPI confidence intervals, and testing more sophisticated LLMs as fine-tuned judges for ARES.\n\n## 7 Limitations\n\nARES relies on a small set of annotations in the human preference validation set (roughly 150-300 datapoints but more is better). These annotations often require an annotator familiar with the RAG system's domain application. While these annotations can be easy to generate for general-domain applications, more specialized domains, such as law, medicine, and finance, may require annotators with specialized expertise.\n\nThe LLMs used in ARES benefit substantially from GPU-based hardware with substantial storage. In ARES, DeBERTa-v3-Large (304M) and FLAN-T5-XXL (11.3B) required GPUs with about 32GB of memory to run, taking several hours for fine-tuning and generation, respectively. While commercial GPUs are widely available, they are not easily accessible to all NLP researchers and practitioners due to their costs.\n\nAdditionally, all of the datasets used in our evaluation of ARES are in English, a well-resourced language with abundant annotations. Future work should explore how ARES can be employed in other languages by utilizing different LLMs for the ARES judge and the synthetic data generation. This can help us better understand the strengths and weaknesses of the current ARES framework.\n\n## References\n\nMubashara Akhtar, Rami Aly, Christos Christodoulopoulos, Oana Cocarascu, Zhijiang Guo, Arpit Mittal, Michael Schlichtkrull, James Thorne, and Andreas Vlachos, editors. 2023. Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER) . Association for Computational Linguistics, Dubrovnik, Croatia.\n\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, and Tijana Zrnic. 2023. Prediction-powered inference.\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\n\nAlec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in retrieval-augmented generation. arXiv preprint arXiv:2309.01431 .\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 .\n\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755 .\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241 .\n\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018) .\n\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166 .\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations.\n\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. Trueteacher: Learning factual consistency evaluation with large language models.\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning , pages 3929-3938. PMLR.\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543 .\n\nJeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 328-339, Melbourne, Australia. Association for Computational Linguistics.\n\nSiqing Huo, Negar Arabzadeh, and Charles LA Clarke. 2023. Retrieving supporting evidence for llms generated answers. arXiv preprint arXiv:2306.13781 .\n\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2019. CodeSearchNet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 .\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 .\n\nJithin James and Shahul Es. 2023. Ragas: Evaluation framework for your retrieval augmented generation (rag) pipelines.\n\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data , 7(3):535-547.\n\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, and Jimmy Lin. 2023. Hagrid: A humanllm collaborative dataset for generative informationseeking with attribution.\n\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense passage retrieval for opendomain question answering.\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages 252-262.\n\nOmar Khattab, Christopher Potts, and Matei Zaharia. 2021. Relevance-guided supervision for openqa with colbert. Transactions of the association for computational linguistics , 9:929-944.\n\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A method for stochastic optimization.\n\nTom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520 .\n\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics , pages 1650-1669, Dubrovnik, Croatia. Association for Computational Linguistics.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:453466.\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459-9474.\n\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. Xglue: A new benchmark dataset for cross-lingual pre-training, understanding and generation. arXiv , abs/2004.01401.\n\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a. G-eval: Nlg evaluation using gpt-4 with better human alignment, may 2023. arXiv preprint arXiv:2303.16634 .\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023b. Calibrating llmbased evaluator. arXiv preprint arXiv:2309.13308 .\n\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented language models: a survey.\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.\n\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2523-2544, Online. Association for Computational Linguistics.\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2022. Measuring attribution in natural language generation models.\n\nJon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.\n\nUdapdr: Unsupervised domain adaptation via llm prompting and distillation of rerankers. arXiv preprint arXiv:2303.00807 .\n\nDavid P Sander and Laura Dietz. 2021. Exam: How to evaluate retrieve-and-generate systems for users who do not (yet) know what they want. In DESIRES , pages 136-146.\n\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 3715-3734, Seattle, United States. Association for Computational Linguistics.\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation.\n\nMosaicML NLP Team. 2023. Introducing mpt-30b: Raising the bar for open-source foundation models. Accessed: 2023-06-22.\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems , 32.\n\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048 .\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 .\n\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models.\n\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885 .\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 .\n\n## A Appendix\n\n## A.1 Fine-tuning Configuration for LLM Judges\n\nFor our loss function used in LLM judge training, we selected cross-entropy loss using Adam\n\n(Kingma and Ba, 2017). For our classification head, we use a single linear classification layer and apply a 0.1 dropout to the input, which is the final hidden state of the [CLS] token. For our learning schedule, we use linear warmup and linear decay (Howard and Ruder, 2018) with a 5e-6 learning rate and a 32 training batch size across all experimental configurations.\n\n## A.2 GPT Prompting for Context Relevance Scoring\n\nFor the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use 8 few-shot examples with the following prompt to score context relevance:\n\n- \u00b7 Given the following question and document, you must analyze the provided document and determine whether it is sufficient for answering the question. In your evaluation, you should consider the content of the document and how it relates to the provided question. Output your final verdict by strictly following this format: \"[[Yes]]\" if the document is sufficient and \"[[No]]\" if the document provided is not sufficient. Do not provide any additional explanation for your decision.\n\nQuestion: <\n\nfew-shot example here >\n\nDocument: < few-shot example here >\n\nFor FEVER, we use the following prompt to score context relevance:\n\n- \u00b7 You are an expert fact-checking agent. Given the following statement and document, you must analyze the provided document and determine whether it is sufficient for determining the statement's factuality. In your evaluation, you should consider the content of the document and how it relates to the provided statement's factuality. Output your final verdict by strictly following this format: \"[[Yes]]\" if the document is sufficient and \"[[No]]\" if the document is not sufficient. Do not provide any additional explanation for your decision.\n\nStatement: < few-shot example here >\n\nDocument: <\n\nfew-shot example here >\n\nFor WoW, we use the following prompt to score context relevance:\n\n- \u00b7 You are an expert dialogue agent. Given the following dialogue and document, you must\n\nanalyze the provided document and determine whether it is relevant for responding to the dialogue. In your evaluation, you should consider the content of the document and how it relates to the provided dialogue. Output your final verdict by strictly following this format: \"[[Yes]]\" if the document is relevant and \"[[No]]\" if the document provided is not relevant. Do not provide any additional explanation for your decision.\n\nDialogue: <\n\nfew-shot example here >\n\nDocument: < few-shot example here >\n\n## A.3 GPT Prompting for Answer Faithfulness Scoring\n\nFor the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use 8 few-shot examples with the following prompt to score answer faithfulness:\n\n- \u00b7 Given the following question, document, and answer, you must analyze the provided answer and determine whether it is faithful to the contents of the document. The answer must not offer new information beyond the context provided in the document. The answer also must not contradict information provided in the document. Output your final verdict by strictly following this format: \"[[Yes]]\" if the answer is faithful to the document and \"[[No]]\" if the answer is not faithful to the document. Do not provide any additional explanation for your decision.\n\nQuestion: <\n\nfew-shot example here >\n\nDocument: <\n\nfew-shot example here >\n\nAnswer: <\n\nfew-shot example here >\n\nFor FEVER, we change the word \"question\" in the prompt to \"statement\". For WoW, we change the word \"question\" in the prompt to \"dialogue\".\n\n## A.4 GPT Prompting for Answer Relevance Scoring\n\nFor the NQ, HotpotQA, MultiRC, and ReCoRD datasets, we use 8 few-shot examples with the following prompt to score answer relevance:\n\n- \u00b7 Given the following question, document, and answer, you must analyze the provided answer and document before determining whether the answer is relevant for the provided question. In your evaluation, you should consider\n\nwhether the answer addresses all aspects of the question and provides only correct information from the document for answering the question. Output your final verdict by strictly following this format: \"[[Yes]]\" if the answer is relevant for the given question and \"[[No]]\" if the answer is not relevant for the given question. Do not provide any additional explanation for your decision.\n\nQuestion: <\n\nfew-shot example here >\n\nDocument: <\n\nfew-shot example here >\n\nAnswer: < few-shot example here >\n\nFor FEVER, we change the word \"question\" in the prompt to \"statement\". For WoW, we change the word \"question\" in the prompt to \"dialogue\".\n\n## A.5 Prompting for Generation of Synthetic Queries and Answers\n\nTo generate synthetic queries and answers using FLAN-T5, we use the following prompt and provide 5 few-shot examples:\n\n- \u2022 Example N\n\nQuestion: < few-shot example here >\n\nDocument: <\n\nfew-shot example here >\n\nAnswer: < few-shot example here >\n\nWe use the same prompting structure for generating incorrect or contradictory answers; we simply swap out the few-shot examples to be incorrect or contradictory instead.\n\n## A.6 Synthetic Query and Answer Generation\n\nFor generating our synthetic questions, we use the following prompt for FLAN-T5 XXL:\n\n- \u2022 Example #1\n\nDocument: < few-shot example here >\n\nQuery: <\n\nfew-shot example here >\n\nExample #2\n\nDocument: <\n\nfew-shot example here >\n\nQuery: <\n\nfew-shot example here >\n\nExample #3\n\nDocument: <\n\nfew-shot example here >\n\nQuery: < few-shot example here >\n\nExample #4\n\nDocument: <\n\nin-domain passage >\n\nQuery:\n\nFor generating our synthetic answers, we use the following prompt for FLAN-T5 XXL:\n\n## \u00b7 Example #1\n\nQuery: < few-shot example here >\n\nDocument: <\n\nfew-shot example here >\n\nAnswer: <\n\nfew-shot example here >\n\nExample #2\n\nQuery: <\n\nfew-shot example here >\n\nDocument: <\n\nfew-shot example here >\n\nAnswer: <\n\nfew-shot example here >\n\nExample #3\n\nQuery: < few-shot example here >\n\nDocument: < few-shot example here >\n\nAnswer: <\n\nfew-shot example here >\n\nExample #4\n\nQuery: < synthetic query here >\n\nDocument: <\n\nin-domain passage here >\n\nAnswer:\n\nFigure 3: RAG Systems Evaluation on NQ - Answer Relevance\n\n<!-- image -->\n\nRAG Framework\n\nFigure 2: RAG Systems Evaluation on NQ - Context Relevance\n\n<!-- image -->\n\nRAG Framework\n\nTable 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall's Tau : The Kendall's tau values represent the correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same experimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human preference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their accuracies in context relevance and answer relevance (C.R. and A.R., respectively).\n\n|                   | Kendall's Tau by Dataset   | Kendall's Tau by Dataset   | Kendall's Tau by Dataset   | Kendall's Tau by Dataset   | Kendall's Tau by Dataset   | Kendall's Tau by Dataset   |\n|-------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|----------------------------|\n|                   | NQ                         | NQ                         | MultiRC                    | MultiRC                    | ReCoRD                     | ReCoRD                     |\n| PPI Labeled Count | C.R.                       | A.R.                       | C.R.                       | A.R.                       | C.R.                       | A.R.                       |\n| 400               | 1.0                        | 1.0                        | 0.89                       | 0.94                       | 0.89                       | 0.94                       |\n| 300               | 0.89                       | 1.0                        | 0.94                       | 0.89                       | 0.83                       | 0.89                       |\n| 200               | 0.83                       | 1.0                        | 0.83                       | 0.94                       | 0.83                       | 0.83                       |\n| 150               | 0.72                       | 1.0                        | 0.83                       | 0.89                       | 0.72                       | 0.83                       |\n| 100               | 0.44                       | 1.0                        | 0.67                       | 0.67                       | 0.67                       | 0.83                       |\n| 50                | 0.44                       | 0.94                       | 0.61                       | 0.44                       | 0.56                       | 0.67                       |\n| 25                | 0.44                       | 0.89                       | 0.56                       | 0.44                       | 0.44                       | 0.56                       |\n\nTable 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated labels instead of human annotations for our human preference validation set in ARES. In the experiments, we generated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3, and A.4). While GPT-4 generated labels decreased Kendall's tau in most settings by 0.05 to 0.30, the ability to cheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of annotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we generate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from the lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM judge (DeBERTa-v3-Large) for evaluation.\n\n|                                         | ARES Ranking of Pseudo RAG Systems using GPT-4 Labels   | ARES Ranking of Pseudo RAG Systems using GPT-4 Labels   | ARES Ranking of Pseudo RAG Systems using GPT-4 Labels   | ARES Ranking of Pseudo RAG Systems using GPT-4 Labels   | ARES Ranking of Pseudo RAG Systems using GPT-4 Labels   | ARES Ranking of Pseudo RAG Systems using GPT-4 Labels   |\n|-----------------------------------------|---------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------|\n|                                         | NQ                                                      | NQ                                                      | ReCoRD                                                  | ReCoRD                                                  | MultiRC                                                 | MultiRC                                                 |\n|                                         | Context Relevance                                       | Answer Relevance                                        | Context Relevance                                       | Answer Relevance                                        | Context Relevance                                       | Answer Relevance                                        |\n| Kendall's Tau                           | 0.78                                                    | 1.0                                                     | 0.78                                                    | 0.72                                                    | 0.89                                                    | 0.78                                                    |\n| Kendall's Tau of Human Labeled Approach | 0.94                                                    | 1.0                                                     | 0.83                                                    | 0.89                                                    | 0.94                                                    | 0.89                                                    |\n| Average PPI Range                       | 9.2%                                                    | 6.8%                                                    | 8.2%                                                    | 9.0%                                                    | 7.7%                                                    | 8.3%                                                    |\n| Accuracy on RAG Evaluation Sets         | 79.3%                                                   | 96.7%                                                   | 88.4%                                                   | 78.3%                                                   | 85.8%                                                   | 82.5%                                                   |\n\nTable 5: ARES Ranking on Real-World RAG Systems : For scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets. Additionally, we include the Kendall's taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n|                                       | ARES Ranking of Real RAG Systems   | ARES Ranking of Real RAG Systems   | ARES Ranking of Real RAG Systems   | ARES Ranking of Real RAG Systems   | ARES Ranking of Real RAG Systems   | ARES Ranking of Real RAG Systems   |\n|---------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|------------------------------------|\n|                                       | NQ                                 | NQ                                 | WoW                                | WoW                                | FEVER                              | FEVER                              |\n|                                       | C.R.                               | A.R.                               | C.R.                               | A.R.                               | C.R.                               | A.R.                               |\n| Kendall's Tau for Sampled Annotations | 0.73                               | 0.78                               | 0.73                               | 0.73                               | 0.73                               | 0.82                               |\n| Kendall's Tau for RAGAS               | 0.82                               | 0.82                               | 0.73                               | 0.82                               | 0.73                               | 0.87                               |\n| Kendall's Tau for GPT-3.5 Judge       | 0.82                               | 0.87                               | 0.82                               | 0.82                               | 0.64                               | 0.87                               |\n| Kendall's Tau for ARES LLM Judge      | 0.91                               | 0.96                               | 0.91                               | 1.0                                | 0.73                               | 0.87                               |\n| Kendall's Tau for ARES                | 1.0                                | 0.96                               | 0.91                               | 1.0                                | 0.82                               | 1.0                                |\n| RAGAS Accuracy                        | 35.9%                              | 68.2%                              | 44.4%                              | 80.1%                              | 21.4%                              | 75.9%                              |\n| GPT-3.5 Accuracy                      | 80.5%                              | 91.2%                              | 81.2%                              | 83.5%                              | 61.3%                              | 54.5%                              |\n| ARES Accuracy                         | 85.6%                              | 93.3%                              | 84.5%                              | 88.2%                              | 70.4%                              | 84.0%                              |\n\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the fine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance (C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and ReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that additional examples further improved the performance of ARES. Furthermore, we found that even in scenarios where the fine-tuned LLM judge's accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of percentage points from the lower bound to the upper bound of the PPI confidence interval.\n\n|                                      | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems   | ARES Cross-Domain Ranking of Pseudo RAG Systems                         | ARES Cross-Domain Ranking of Pseudo RAG Systems   |\n|--------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|---------------------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------|\n|                                      | NQ to FEVER                                       | NQ to FEVER                                       | FEVER to NQ                                       | FEVER to NQ                                       | NQ to MultiRC                                     | NQ to MultiRC                                     | MultiRC to NQ                                     | MultiRC to NQ                                     | NQ to ReCoRD                                      | NQ to ReCoRD                                      | ReCoRD to NQ                                                            | ReCoRD to NQ                                      |\n|                                      | C.R.                                              | A.R.                                              | C.R.                                              | A.R.                                              | C.R.                                              | A.R.                                              | C.R.                                              | A.R.                                              | C.R.                                              | A.R.                                              | C.R.                                                                    | A.R.                                              |\n| Kendall's Tau                        | 0.89                                              | 0.89                                              | 1.0                                               | 0.83                                              | 0.94                                              | 0.89                                              | 1.0                                               | 0.89                                              | 0.78                                              | 0.89                                              | 0.89                                                                    | 0.94                                              |\n| Kendall's Tau of In-Domain LLM Judge | 0.89                                              | 0.78                                              | 0.94                                              | 1.0                                               | 0.94                                              | 0.89                                              | 0.94                                              | 1.0                                               | 0.83                                              | 0.89                                              | 0.94                                                                    | 1.0                                               |\n| Average PPI Range                    | 8.7%                                              | 7.2%                                              |                                                   |                                                   |                                                   |                                                   | 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1%    |                                                   |                                                   |                                                   | 9.7%                                                                    | 6.2%                                              |\n| Accuracy on RAG Evaluation Sets      |                                                   |                                                   |                                                   |                                                   |                                                   |                                                   |                                                   |                                                   |                                                   |                                                   | 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1% |                                                   |\n\nTable 7: Positive and Negatives Evaluation Examples\n\n| Query                                                                                | Passage                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Answer                                                        |   Context Relevance |   Answer Relevance |\n|--------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|---------------------|--------------------|\n| How can a ball that is not moving possess energy of position?                        | Mechanical energy is a combination of the energy of motion or position. This type of energy describes objects that are moving or could move. A moving ball can have energy from motion. An arrow can also have the energy of motion. Both are types of mechanical energy.                                                                                                                                                                                                                                                                                                                                                                                                                       | The ball holds mechanical energy                              |                   1 |                  1 |\n| Who has a Jimmy Stewart-like quality of quiet trust?                                 | Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust. In black jeans and button-down shirt, he's a kind of folk hero in the south Bethlehem melting pot where he's crafted a law practice catering to working-class families - mostly Latino - in the shadow of the hulkish remnants of Bethlehem Steel.                                                                                                                                                                                                                                                                                                                                                                                | Fred Rooney                                                   |                   1 |                  1 |\n| Before he murder the doctor and Ralph Smith, where did the stepfather reside?        | the stepfather has been institutionalized in Puget Sound, Washington since , spending his time building model houses in the workshop. Assigned a new doctor named Joseph Danvers the stepfather begins confiding in him to gain his trust , ultimately murdering the doctor during a session by stabbing him in the neck with a blade smuggled out of the workshop . After killing Danvers the stepfather beats a suspicious guard named Ralph Smith to death with his own nightstick with only two strikes and takes his uniform , successfully sneaking out of the sanitarium . Checking into a hotel after robbing and murdering a traveling salesman the stepfather alters his appearance , | Los Angeles                                                   |                   1 |                  0 |\n| What was the name of the 2006 film about Pushkin's death, and who portrayed Pushkin? | Times, and a performance of Carmen at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met the president of Columbia University, who described Einstein as \"The ruling monarch of the mind.\" Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance.                                                                                                                                                                                  | Vasily Szaitsev portrayed Pushkin in the film Pushkin Returns |                   0 |                  0 |", "title": "ARES_An_Automated_Evaluation_Framework_for_Retrieval-Augmented_Generation_Systems", "expert_id": "67847c5880957f028e351613", "link": "https://arxiv.org/pdf/2311.09476", "published_at": "2023-11-16 00:39:39"}]